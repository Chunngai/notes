{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define the network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines an nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # n_c^(l - 1), n_c^(l), f\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        \n",
    "        # y = Wx + b\n",
    "        # in-dimension, out-d\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # backward func: automatically defined using autograd\n",
    "        \n",
    "#         x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "#         # If the size is a square you can only specify a single number\n",
    "#         x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        \n",
    "#         x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "\n",
    "        z1 = self.conv1(x)\n",
    "        a1 = F.relu(z1)\n",
    "        a1 = F.max_pool2d(a1, (2, 2))\n",
    "        \n",
    "        z2 = self.conv2(a1)\n",
    "        a2 = F.relu(z2)\n",
    "        a2 = F.max_pool2d(a2, 2)\n",
    "        a2 = a2.view(-1, self.num_flat_features(a2))\n",
    "        \n",
    "        z3 = self.fc1(a2)\n",
    "        a3 = F.relu(z3)\n",
    "        \n",
    "        z4 = self.fc2(a3)\n",
    "        a4 = F.relu(z4)\n",
    "        \n",
    "        a5 = self.fc3(a4)\n",
    "        \n",
    "        return a5\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 3, 3])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 576])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "\n",
    "print(len(params))\n",
    "for param in params:\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10  \n",
    "  \n",
    "torch.Size([6, 1, 3, 3])  # W.size = (n_c^(l + 1), n_c^(l), f_h, f_w)  \n",
    "torch.Size([6])  # b  \n",
    "torch.Size([16, 6, 3, 3])  \n",
    "torch.Size([16])  \n",
    "  \n",
    "torch.Size([120, 576])  # W  \n",
    "torch.Size([120])  # b  \n",
    "torch.Size([84, 120])  \n",
    "torch.Size([84])  \n",
    "torch.Size([10, 84])  \n",
    "torch.Size([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processes inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0624,  0.0130,  0.0016,  0.1018, -0.0311,  0.0188, -0.0906,  0.0009,\n",
      "         -0.0091, -0.1172]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero the gradient buffers of all parameters\n",
    "net.zero_grad()\n",
    "# inits with random gradients\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Loss Function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4273, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.rand(10).view(1, -1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7f0429b1d0d0>\n",
      "<AddmmBackward object at 0x7f0429b1d090>\n",
      "<AccumulateGrad object at 0x7f0429b0b5d0>\n"
     ]
    }
   ],
   "source": [
    "# when we call loss.backward(), the whole graph is differentiated \n",
    "# w.r.t. the loss, and all Tensors in the graph that has \n",
    "# requires_grad=True will have their .grad Tensor **accumulated** \n",
    "# with the gradient.\n",
    "print(loss.grad_fn)\n",
    "print(loss.grad_fn.next_functions[0][0])\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outputs of the next_functions can be:  \n",
    "&nbsp;&nbsp;&nbsp;(<AccumulateGrad object at 0x7fb6451fa2d0>, 0): learning param leaves  \n",
    "&nbsp;&nbsp;&nbsp;(None, 0): const leaves  \n",
    "&nbsp;&nbsp;&nbsp;(<MulBackward0 object at 0x7fb6451fa210>, 0): operations  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Backprop</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to clear the existing gradients though, \n",
    "# else gradients will be accumulated to existing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]])\n",
      "tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "tensor([[[[-0.0070, -0.0122,  0.0065],\n",
      "          [-0.0049, -0.0035,  0.0107],\n",
      "          [ 0.0119,  0.0020,  0.0184]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0022,  0.0037,  0.0092],\n",
      "          [-0.0059,  0.0076, -0.0127],\n",
      "          [-0.0161, -0.0090,  0.0129]]],\n",
      "\n",
      "\n",
      "        [[[-0.0085,  0.0082, -0.0024],\n",
      "          [-0.0195,  0.0021,  0.0045],\n",
      "          [ 0.0043,  0.0076, -0.0087]]],\n",
      "\n",
      "\n",
      "        [[[-0.0002, -0.0053,  0.0212],\n",
      "          [-0.0151,  0.0044, -0.0041],\n",
      "          [-0.0108, -0.0052,  0.0013]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0027,  0.0041, -0.0040],\n",
      "          [-0.0023,  0.0057, -0.0040],\n",
      "          [ 0.0047,  0.0022, -0.0138]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0014, -0.0005, -0.0190],\n",
      "          [ 0.0122,  0.0039,  0.0104],\n",
      "          [ 0.0084,  0.0059, -0.0107]]]])\n",
      "tensor([[[[-0.0070, -0.0122,  0.0065],\n",
      "          [-0.0049, -0.0035,  0.0107],\n",
      "          [ 0.0119,  0.0020,  0.0184]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0022,  0.0037,  0.0092],\n",
      "          [-0.0059,  0.0076, -0.0127],\n",
      "          [-0.0161, -0.0090,  0.0129]]],\n",
      "\n",
      "\n",
      "        [[[-0.0085,  0.0082, -0.0024],\n",
      "          [-0.0195,  0.0021,  0.0045],\n",
      "          [ 0.0043,  0.0076, -0.0087]]],\n",
      "\n",
      "\n",
      "        [[[-0.0002, -0.0053,  0.0212],\n",
      "          [-0.0151,  0.0044, -0.0041],\n",
      "          [-0.0108, -0.0052,  0.0013]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0027,  0.0041, -0.0040],\n",
      "          [-0.0023,  0.0057, -0.0040],\n",
      "          [ 0.0047,  0.0022, -0.0138]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0014, -0.0005, -0.0190],\n",
      "          [ 0.0122,  0.0039,  0.0104],\n",
      "          [ 0.0084,  0.0059, -0.0107]]]])\n",
      "tensor([ 0.0105,  0.0036,  0.0098,  0.0035,  0.0016, -0.0004])\n",
      "tensor([ 0.0105,  0.0036,  0.0098,  0.0035,  0.0016, -0.0004])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "\n",
    "print(net.conv1.weight.grad)\n",
    "print(list(net.parameters())[0].grad)\n",
    "\n",
    "print(net.conv1.bias.grad)\n",
    "print(list(net.parameters())[1].grad)\n",
    "\n",
    "print()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(net.conv1.weight.grad)\n",
    "print(list(net.parameters())[0].grad)\n",
    "\n",
    "print(net.conv1.bias.grad)\n",
    "print(list(net.parameters())[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Update the weights</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updates the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in the training loop!\n",
    "\n",
    "# forward prop\n",
    "output = net(input)\n",
    "\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# back prop\n",
    "optimizer.zero_grad()  # zeroes the gradient buffers!\n",
    "# Observe how gradient buffers had to be manually set to zero \n",
    "# using optimizer.zero_grad(). This is because gradients are \n",
    "# accumulated\n",
    "loss.backward()\n",
    "\n",
    "# updates params using gd\n",
    "# Performs a single optimization step.\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
