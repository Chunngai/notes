{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define the network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines an nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # n_c^(l - 1), n_c^(l), f\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        \n",
    "        # y = Wx + b\n",
    "        # in-dimension, out-d\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # backward func: automatically defined using autograd\n",
    "        \n",
    "#         x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "#         # If the size is a square you can only specify a single number\n",
    "#         x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        \n",
    "#         x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "\n",
    "        z1 = self.conv1(x)\n",
    "        a1 = F.relu(z1)\n",
    "        a1 = F.max_pool2d(a1, (2, 2))\n",
    "        \n",
    "        z2 = self.conv2(a1)\n",
    "        a2 = F.relu(z2)\n",
    "        a2 = F.max_pool2d(a2, 2)\n",
    "        a2 = a2.view(-1, self.num_flat_features(a2))\n",
    "        \n",
    "        z3 = self.fc1(a2)\n",
    "        a3 = F.relu(z3)\n",
    "        \n",
    "        z4 = self.fc2(a3)\n",
    "        a4 = F.relu(z4)\n",
    "        \n",
    "        a5 = self.fc3(a4)\n",
    "        \n",
    "        return a5\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 3, 3])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 576])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "\n",
    "print(len(params))\n",
    "for param in params:\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10  \n",
    "  \n",
    "torch.Size([6, 1, 3, 3])  # W.size = (n_c^(l + 1), n_c^(l), f_h, f_w)  \n",
    "torch.Size([6])  # b  \n",
    "torch.Size([16, 6, 3, 3])  \n",
    "torch.Size([16])  \n",
    "  \n",
    "torch.Size([120, 576])  # W  \n",
    "torch.Size([120])  # b  \n",
    "torch.Size([84, 120])  \n",
    "torch.Size([84])  \n",
    "torch.Size([10, 84])  \n",
    "torch.Size([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processes inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0992,  0.0480, -0.0560,  0.0302,  0.0168, -0.0013,  0.0642,  0.0088,\n",
      "         -0.0238,  0.0475]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero the gradient buffers of all parameters\n",
    "net.zero_grad()\n",
    "# inits with random gradients\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Loss Function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4194, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.rand(10).view(1, -1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7fe7696842d0>\n",
      "<AddmmBackward object at 0x7fe769684490>\n",
      "<AccumulateGrad object at 0x7fe7696842d0>\n"
     ]
    }
   ],
   "source": [
    "# when we call loss.backward(), the whole graph is differentiated \n",
    "# w.r.t. the loss, and all Tensors in the graph that has \n",
    "# requires_grad=True will have their .grad Tensor **accumulated** \n",
    "# with the gradient.\n",
    "print(loss.grad_fn)\n",
    "print(loss.grad_fn.next_functions[0][0])\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outputs of the next_functions can be:  \n",
    "&nbsp;&nbsp;&nbsp;(<AccumulateGrad object at 0x7fb6451fa2d0>, 0): learning param leaves  \n",
    "&nbsp;&nbsp;&nbsp;(None, 0): const leaves  \n",
    "&nbsp;&nbsp;&nbsp;(<MulBackward0 object at 0x7fb6451fa210>, 0): operations  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Backprop</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to clear the existing gradients though, \n",
    "# else gradients will be accumulated to existing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]])\n",
      "tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "tensor([[[[-6.3048e-03,  3.8273e-03, -1.5049e-02],\n",
      "          [-2.1484e-03,  2.9954e-03, -2.1047e-03],\n",
      "          [ 1.1542e-02,  3.6834e-03,  1.0766e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.0218e-02, -4.4687e-03, -1.3140e-02],\n",
      "          [ 8.8471e-05,  2.3798e-02, -5.4381e-03],\n",
      "          [-9.0605e-04,  1.0833e-03, -1.1258e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 8.3735e-03,  3.3284e-03,  6.6910e-03],\n",
      "          [-7.8934e-03, -2.3659e-03,  3.4525e-03],\n",
      "          [-5.8542e-03,  8.9853e-03,  7.0149e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.5371e-03,  7.1270e-03, -3.1072e-03],\n",
      "          [ 3.0059e-03, -6.1029e-03, -3.8761e-03],\n",
      "          [ 4.2632e-03, -4.6602e-03, -3.1681e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.6787e-03, -8.1398e-03, -5.9812e-03],\n",
      "          [ 1.2479e-04,  1.0992e-02, -5.4354e-03],\n",
      "          [ 1.3740e-02, -3.6978e-03, -6.1328e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 3.8969e-03,  1.3350e-03, -3.2440e-03],\n",
      "          [ 6.9335e-03, -1.4232e-02,  2.4559e-03],\n",
      "          [-2.9865e-03,  6.7534e-03, -4.5371e-03]]]])\n",
      "tensor([[[[-6.3048e-03,  3.8273e-03, -1.5049e-02],\n",
      "          [-2.1484e-03,  2.9954e-03, -2.1047e-03],\n",
      "          [ 1.1542e-02,  3.6834e-03,  1.0766e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.0218e-02, -4.4687e-03, -1.3140e-02],\n",
      "          [ 8.8471e-05,  2.3798e-02, -5.4381e-03],\n",
      "          [-9.0605e-04,  1.0833e-03, -1.1258e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 8.3735e-03,  3.3284e-03,  6.6910e-03],\n",
      "          [-7.8934e-03, -2.3659e-03,  3.4525e-03],\n",
      "          [-5.8542e-03,  8.9853e-03,  7.0149e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.5371e-03,  7.1270e-03, -3.1072e-03],\n",
      "          [ 3.0059e-03, -6.1029e-03, -3.8761e-03],\n",
      "          [ 4.2632e-03, -4.6602e-03, -3.1681e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.6787e-03, -8.1398e-03, -5.9812e-03],\n",
      "          [ 1.2479e-04,  1.0992e-02, -5.4354e-03],\n",
      "          [ 1.3740e-02, -3.6978e-03, -6.1328e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 3.8969e-03,  1.3350e-03, -3.2440e-03],\n",
      "          [ 6.9335e-03, -1.4232e-02,  2.4559e-03],\n",
      "          [-2.9865e-03,  6.7534e-03, -4.5371e-03]]]])\n",
      "tensor([-3.9179e-03, -1.0748e-02, -2.8425e-03,  4.7715e-06, -8.0766e-03,\n",
      "        -1.4794e-03])\n",
      "tensor([-3.9179e-03, -1.0748e-02, -2.8425e-03,  4.7715e-06, -8.0766e-03,\n",
      "        -1.4794e-03])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "\n",
    "print(net.conv1.weight.grad)\n",
    "print(list(net.parameters())[0].grad)\n",
    "\n",
    "print(net.conv1.bias.grad)\n",
    "print(list(net.parameters())[1].grad)\n",
    "\n",
    "print()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(net.conv1.weight.grad)\n",
    "print(list(net.parameters())[0].grad)\n",
    "\n",
    "print(net.conv1.bias.grad)\n",
    "print(list(net.parameters())[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Update the weights</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updates the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in the training loop!\n",
    "\n",
    "# forward prop\n",
    "output = net(input)\n",
    "\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# back prop\n",
    "optimizer.zero_grad()  # zeroes the gradient buffers!\n",
    "# Observe how gradient buffers had to be manually set to zero \n",
    "# using optimizer.zero_grad(). This is because gradients are \n",
    "# accumulated\n",
    "loss.backward()\n",
    "\n",
    "# updates params using gd\n",
    "# Performs a single optimization step.\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
