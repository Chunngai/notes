{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "SRC = Field(tokenize=\"spacy\", \n",
    "            tokenizer_language=\"de\", \n",
    "            init_token=\"<sos>\", \n",
    "            eos_token=\"<eos>\", \n",
    "            lower=True)\n",
    "\n",
    "TRG = Field(tokenize=\"spacy\", \n",
    "            tokenizer_language=\"en\", \n",
    "            init_token=\"<sos>\", \n",
    "            eos_token=\"<eos>\", \n",
    "            lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts=(\".de\", \".en\"), \n",
    "                                                    fields=(SRC, TRG))\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "# if batch size is too large: cuda out of memory\n",
    "# if batch size is too small: much time\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    datasets=(train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device)\n",
    "# when len of sentences are diff, use <pad>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                input_dim: int, \n",
    "                emb_dim: int,\n",
    "                enc_hid_dim: int,\n",
    "                dec_hid_dim: int,\n",
    "                dropout: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)  # avoids overfitting when the dataset is small\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)  # bi-gru\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)  # the last hiddens of the encoder will be the first hidden of the decoder\n",
    "                \n",
    "    def forward(self, src: Tensor) -> Tuple[Tensor]:\n",
    "        # (in)  src:      [seq_len, batch]\n",
    "        # (out) embedded: [seq_len, batch, emb_dim]\n",
    "        embedded = self.embedding(src)\n",
    "        # (int, out) embedded: [seq_len, batch, emb_dim]\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # (in)  embedded: [seq_len, batch, emb_dim]\n",
    "        # (in)  hidden:   [2, batch, enc_hid_dim]\n",
    "        # (out) outputs:  [seq_len, batch, 2 * enc_hid_dim]\n",
    "        # (out) hidden:   [2, batch, enc_hid_dim]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        \n",
    "        last_forward_hidden = hidden[0]  # hidden[0, :, :] == hidden[0]. the last forward\n",
    "        last_backward_hidden = hidden[1]  # hidden[1, :, :] == hidden[1]. the last backward\n",
    "        # (in)  last_forward_hidden:  [batch, enc_hid_dim]\n",
    "        # (in)  last_backward_hidden  [batch, enc_hid_dim]\n",
    "        # (out) last_hidden:          [batch, enc_hid_dim * 2]                 \n",
    "        last_hidden = torch.cat((last_forward_hidden, last_backward_hidden), dim=1)\n",
    "        # (in)  last_hidden: [batch, enc_hid_dim * 2]\n",
    "        # (out) last_hidden: [batch, dec_hid_dim]\n",
    "        last_hidden = self.fc(last_hidden)\n",
    "        # (in)  last_hidden: [batch, dec_hid_dim]\n",
    "        # (out) last_hidden: [batch, dec_hid_dim]\n",
    "        last_hidden = torch.tanh(last_hidden)\n",
    "        \n",
    "        return outputs, last_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, \n",
    "                enc_hid_dim: int, \n",
    "                dec_hid_dim: int,\n",
    "                attn_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.attn_dim = attn_dim\n",
    "        \n",
    "        # forward enc hid + backward enc hid + dec hid\n",
    "        self.attn_in_dim = (enc_hid_dim * 2) + dec_hid_dim\n",
    "        \n",
    "        self.attn = nn.Linear(self.attn_in_dim, attn_dim)\n",
    "        self.v = nn.Parameter(torch.rand(attn_dim))\n",
    "        \n",
    "    def forward(self, \n",
    "               decoder_hidden: Tensor, \n",
    "               encoder_outputs: Tensor) -> Tensor:\n",
    "        \n",
    "        seq_len = encoder_outputs.shape[0]\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        \n",
    "        # (in)  decoder_hidden: [batch, dec_hid_dim]\n",
    "        # (out) decoder_hidden: [batch, 1, dec_hid_dim]\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1)\n",
    "        # (in)  repeated_decoder_hidden: [batch, 1, dec_hid_dim]\n",
    "        # (out) repeated_decoder_hidden: [batch, seq_len, dec_hid_dim]\n",
    "        repeated_decoder_hidden = decoder_hidden.repeat(1, seq_len, 1)  # repeats this tensor along the specified dimensions.\n",
    "        \n",
    "        # (in)  encoder_outputs: [seq_len, batch, enc_hid_dim * 2]\n",
    "        # (out) encoder_outputs: [batch, seq_len, enc_hid_dim * 2]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        # (in)  repeated_decoder_hidden: [batch, seq_len, dec_hid_dim]\n",
    "        # (in)  encoder_outputs:         [batch, seq_len, enc_hid_dim * 2]\n",
    "        # (out) energy_in:               [batch, seq_len, enc_hid_dim * 2 + dec_hid_dim]\n",
    "        energy_in = torch.cat((repeated_decoder_hidden, encoder_outputs), dim=2)\n",
    "        # (in)  energy_in: [batch, seq_len, enc_hid_dim * 2 + dec_hid_dim]\n",
    "        # (out) energy_in: [batch, seq_len, attn_dim] \n",
    "        energy_in = self.attn(energy_in)\n",
    "        # (in)  energy_in: [batch, seq_len, attn_dim]\n",
    "        # (out) energy:    [batch, seq_len, attn_dim]\n",
    "        energy = torch.tanh(energy_in)\n",
    "        \n",
    "        # (in)  energy: [batch, seq_len, attn_dim]\n",
    "        # (out) energy: [batch, attn_dim, seq_len]\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        # (in)  self.v: [attn_dim]\n",
    "        # (out) v: [batch, attn_dim]\n",
    "        v = self.v.repeat(batch_size, 1)\n",
    "        # (in)  v: [batch, attn_dim]\n",
    "        # (out) v: [batch, 1, attn_dim]\n",
    "        v = v.unsqueeze(1)\n",
    "        # (in)  v:         [batch, 1, attn_dim]\n",
    "        # (in)  energy:    [batch, attn_dim, seq_len]\n",
    "        # (out) attention: [batch, 1, seq_len]\n",
    "        attention = torch.bmm(v, energy)\n",
    "        # (in)  attention: [batch, 1, seq_len]\n",
    "        # (out) attention: [batch, seq_len]\n",
    "        attention = attention.squeeze(1)\n",
    "        # (in)  attention: [batch, seq_len]\n",
    "        # (out) attention: [batch, seq_len]\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        \n",
    "        return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                output_dim: int, \n",
    "                emb_dim: int,\n",
    "                enc_hid_dim: int, \n",
    "                dec_hid_dim, \n",
    "                dropout: int,\n",
    "                attention: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.out = nn.Linear(enc_hid_dim * 2 + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "    def forward(self, \n",
    "               input_: Tensor, \n",
    "               decoder_hidden: Tensor, \n",
    "               encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
    "        \n",
    "        # (in)  input_: [batch]\n",
    "        # (out) input_: [1, batch]\n",
    "        input_ = input_.unsqueeze(0)\n",
    "        # (in)  input:    [1, batch]\n",
    "        # (out) embedded: [1, batch, emb_dim]\n",
    "        embedded = self.embedding(input_)\n",
    "        # (in)  embedded: [1, batch, emb_dim]\n",
    "        # (out) embedded: [1, batch, emb_dim]\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # (out) a: (batch, seq_len)\n",
    "        a = self.attention(decoder_hidden, encoder_outputs)\n",
    "        \n",
    "        # (in): a: [batch, seq_len]\n",
    "        # (out) a: [batch, 1, seq_len]\n",
    "        a = a.unsqueeze(1)\n",
    "        # (in)  encoder_outputs: [seq_len, batch, enc_hid_dim * 2]\n",
    "        # (out) encoder_outputs: [batch, seq_len, enc_hid_dim * 2]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # (in)  a:               [batch, 1, seq_len]\n",
    "        # (in)  encoder_outputs: [batch, seq_len, enc_hid_dim * 2]\n",
    "        # (out) weighted:        [batch, 1, enc_hid_dim * 2]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        # (in)  weighted: [batch, 1, enc_hid_dim * 2]\n",
    "        # (out) weighted: [1, batch, enc_hid_dim * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        # (in)  embedded:  [1, batch, emb_dim]\n",
    "        # (in)  weighted:  [1, batch, enc_hid_dim * 2]\n",
    "        # (out) rnn_input: [1, batch, emb_dim + enc_hid_dim * 2]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        # (in)  decoder_hidden: [batch, dec_hid_dim]\n",
    "        # (out) decoder_hidden: [1, batch, dec_hid_dim]\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(0)\n",
    "        # (in)  rnn_input:      [1, batch, emb_dim + enc_hid_dim * 2]\n",
    "        # (in)  decoder_hidden: [1, batch, dec_hid_dim]\n",
    "        # (out) output:         [1, batch, dec_hid_dim]\n",
    "        # (out) decoder_hidden: [1, batch, dec_hid_dim]\n",
    "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden)\n",
    "        \n",
    "        # (in)  embedded: [1, batch, emb_dim]\n",
    "        # (out) embedded: [batch, emb_dim]\n",
    "        embedded = embedded.squeeze(0)\n",
    "        # (in)  output:   [1, batch, dec_hid_dim]\n",
    "        # (out) output:   [batch, dec_hid_dim]\n",
    "        output = output.squeeze(0)\n",
    "        # (in)  weighted: [1, batch, enc_hid_dim * 2]\n",
    "        # (out) weighted: [batch, enc_hid_dim * 2]\n",
    "        weighted = weighted.squeeze(0)\n",
    "        # (in)  output:    [batch, dec_hid_dim]\n",
    "        # (in)  weighted:  [batch, enc_hid_dim * 2]\n",
    "        # (in)  embedded:  [batch, emb_dim]\n",
    "        # (out) output_in: [batch, dec_hid_dim + enc_hid_dim * 2 + emb_dim]\n",
    "        output_in = torch.cat((output, weighted, embedded), dim=1)\n",
    "        # (in)  output_in: [batch, dec_hid_dim + enc_hid_dim * 2 + emb_dim]\n",
    "        # (out) output:    [batch, output_dim]\n",
    "        output = self.out(output_in)\n",
    "        \n",
    "        # (in)  decoder_hidden: [1, batch, dec_hid_dim]\n",
    "        # (out) decoder_hidden: [batch, dec_hid_dim]\n",
    "        decoder_hidden = decoder_hidden.squeeze(0)\n",
    "        \n",
    "        return output, decoder_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                encoder: nn.Module, \n",
    "                decoder: nn.Module,\n",
    "                device: torch.device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, \n",
    "               src: Tensor, \n",
    "               trg: Tensor, \n",
    "               teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
    "        \n",
    "        # Encode.\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        trg_len = trg.shape[0]\n",
    "        batch_size = src.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size, device=device)\n",
    "                \n",
    "        output = trg[0]  # <sos>\n",
    "        # the first col of outputs should be <sos>? no if \n",
    "        for t in range(1, trg_len):  # counts from 1\n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            output = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ATTN_DIM = 64\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(INPUT_DIM, \n",
    "              ENC_EMB_DIM, \n",
    "              ENC_HID_DIM, \n",
    "              DEC_HID_DIM, \n",
    "              ENC_DROPOUT)\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, \n",
    "                 DEC_HID_DIM, \n",
    "                 ATTN_DIM)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              DEC_EMB_DIM, \n",
    "              ENC_HID_DIM, \n",
    "              DEC_HID_DIM, \n",
    "              DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7855, 256)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1536, out_features=64, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (rnn): GRU(1280, 512)\n",
       "    (out): Linear(in_features=1792, out_features=5893, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "# Applies fn recursively to every submodule (as returned by .children()) as well \n",
    "# as self. Typical use includes initializing the parameters of a model \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 19,829,893 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: when scoring the performance of a language translation model in \n",
    "# particular, we have to tell the nn.CrossEntropyLoss function to ignore the \n",
    "# indices where the target is simply padding.\n",
    "PAD_IDX = TRG.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "# __init__ of Field contains: pad_token='<pad>', unk_token='<unk>'\n",
    "# print(SRC.vocab.itos[0], SRC.vocab.itos[1], SRC.vocab.itos[2], SRC.vocab.itos[3])\n",
    "# print(TRG.vocab.itos[0], TRG.vocab.itos[1], TRG.vocab.itos[2], TRG.vocab.itos[3])\n",
    "# <unk> <pad> <sos> <eos>\n",
    "# <unk> <pad> <sos> <eos>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, \n",
    "         iterator: BucketIterator, \n",
    "         optimizer: optim.Optimizer, \n",
    "         criterion: nn.Module, \n",
    "         clip: float):\n",
    "    \n",
    "    # Sets the module in training mode. This has any effect only on certain \n",
    "    # modules. See documentations of particular modules for details of their \n",
    "    # behaviors in training/evaluation mode, if they are affected, e.g. Dropout, \n",
    "    # BatchNorm, etc.\n",
    "    model.train()\n",
    "    \n",
    "    eposh_loss = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        # i, batch\n",
    "        \n",
    "        # Zeros grad.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Data.\n",
    "#         [torchtext.data.batch.Batch of size 32 from MULTI30K]\n",
    "#         [.src]:[torch.cuda.LongTensor of size 24x32 (GPU 0)]\n",
    "#         [.trg]:[torch.cuda.LongTensor of size 25x32 (GPU 0)]\n",
    "        src = batch.src\n",
    "        trg = batch.trg \n",
    "        \n",
    "        # Forward.\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        # Loss.\n",
    "        output = output[1:].view(-1, output.shape[-1])  # output[0]: <sos>\n",
    "        trg = trg[1:].view(-1)  # trg[0]: <sos>\n",
    "        # Input: (N, C) where C = number of classes\n",
    "        # Target: (N) where each value is 0 ≤targets[i]≤ C−1\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        # Backward.\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Updates params.\n",
    "        optimizer.step()\n",
    "        \n",
    "        eposh_loss += loss.item()\n",
    "        \n",
    "    return eposh_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, \n",
    "            iterator: BucketIterator, \n",
    "            criterion: nn.Module):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            \n",
    "            # Data.\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            \n",
    "            # Forward.\n",
    "            output = model(src, trg, 0)  # turns off teacher forcing\n",
    "            \n",
    "            # Loss.\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time: int, \n",
    "              end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time % 60)\n",
    "    \n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 15m 35s\n",
      "\tTrain Loss: 5.321 | Train PPL: 204.607\n",
      "\t Val. Loss: 4.957 |  Val. PPL: 142.120\n",
      "Epoch: 02 | Time: 16m 43s\n",
      "\tTrain Loss: 4.517 | Train PPL:  91.577\n",
      "\t Val. Loss: 4.904 |  Val. PPL: 134.838\n",
      "Epoch: 03 | Time: 15m 31s\n",
      "\tTrain Loss: 4.190 | Train PPL:  66.015\n",
      "\t Val. Loss: 4.766 |  Val. PPL: 117.490\n",
      "Epoch: 04 | Time: 15m 53s\n",
      "\tTrain Loss: 3.906 | Train PPL:  49.715\n",
      "\t Val. Loss: 4.496 |  Val. PPL:  89.651\n",
      "Epoch: 05 | Time: 16m 25s\n",
      "\tTrain Loss: 3.554 | Train PPL:  34.958\n",
      "\t Val. Loss: 4.014 |  Val. PPL:  55.368\n",
      "Epoch: 06 | Time: 16m 49s\n",
      "\tTrain Loss: 3.176 | Train PPL:  23.961\n",
      "\t Val. Loss: 3.697 |  Val. PPL:  40.319\n",
      "Epoch: 07 | Time: 19m 8s\n",
      "\tTrain Loss: 2.852 | Train PPL:  17.322\n",
      "\t Val. Loss: 3.544 |  Val. PPL:  34.618\n",
      "Epoch: 08 | Time: 19m 13s\n",
      "\tTrain Loss: 2.583 | Train PPL:  13.235\n",
      "\t Val. Loss: 3.448 |  Val. PPL:  31.434\n",
      "Epoch: 09 | Time: 15m 58s\n",
      "\tTrain Loss: 2.342 | Train PPL:  10.401\n",
      "\t Val. Loss: 3.314 |  Val. PPL:  27.504\n",
      "Epoch: 10 | Time: 15m 34s\n",
      "\tTrain Loss: 2.163 | Train PPL:   8.699\n",
      "\t Val. Loss: 3.279 |  Val. PPL:  26.536\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training and validating.\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f\"Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.272 | Test PPL:  26.361 |\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
