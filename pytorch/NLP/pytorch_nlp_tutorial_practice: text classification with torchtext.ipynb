{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from torchtext.utils import extract_archive, unicode_csv_reader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets.text_classification import *\n",
    "from torchtext.datasets.text_classification import _csv_iterator,_create_data_from_iterator\n",
    "\n",
    "def _setup_datasets(dataset_name, dataset_tar, root='.data', ngrams=1, vocab=None, include_unk=False):\n",
    "#     dataset_tar = download_from_url(URLS[dataset_name], root=root)\n",
    "    extracted_files = extract_archive(dataset_tar)\n",
    "\n",
    "    for fname in extracted_files:\n",
    "        if fname.endswith('train.csv'):\n",
    "            train_csv_path = fname\n",
    "        if fname.endswith('test.csv'):\n",
    "            test_csv_path = fname\n",
    "\n",
    "    if vocab is None:\n",
    "        logging.info('Building Vocab based on {}'.format(train_csv_path))\n",
    "        vocab = build_vocab_from_iterator(_csv_iterator(train_csv_path, ngrams))\n",
    "    else:\n",
    "        if not isinstance(vocab, Vocab):\n",
    "            raise TypeError(\"Passed vocabulary is not of type Vocab\")\n",
    "    logging.info('Vocab has {} entries'.format(len(vocab)))\n",
    "    logging.info('Creating training data')\n",
    "    train_data, train_labels = _create_data_from_iterator(\n",
    "        vocab, _csv_iterator(train_csv_path, ngrams, yield_cls=True), include_unk)\n",
    "    logging.info('Creating testing data')\n",
    "    test_data, test_labels = _create_data_from_iterator(\n",
    "        vocab, _csv_iterator(test_csv_path, ngrams, yield_cls=True), include_unk)\n",
    "    if len(train_labels ^ test_labels) > 0:\n",
    "        raise ValueError(\"Training and test labels don't match\")\n",
    "    return (TextClassificationDataset(vocab, train_data, train_labels),\n",
    "            TextClassificationDataset(vocab, test_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "def split_train_set(train_set):\n",
    "    train_set_size = int(len(train_set) * 0.80)\n",
    "    val_set_size = len(train_set) - train_set_size\n",
    "    \n",
    "    return random_split(train_set, [train_set_size, val_set_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset_name, dataset_tar, ngrams=1):\n",
    "    print(\"Preparing data...\")\n",
    "    \n",
    "    # Gets train and test sets.\n",
    "    train_set, test_set = _setup_datasets(dataset_name=dataset_name, dataset_tar=dataset_tar, ngrams=ngrams)\n",
    "    \n",
    "    vocab_size = len(train_set.get_vocab())\n",
    "    cls_num = len(train_set.get_labels())\n",
    "    \n",
    "    # Splits train set into train set and val set.\n",
    "    train_set, val_set = split_train_set(train_set)\n",
    "    \n",
    "    return train_set, val_set, test_set, vocab_size, cls_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, cls_num):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)  # sparse=True: significantly increase speed\n",
    "        \n",
    "        self.fc = nn.Linear(embed_dim, cls_num)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, texts, offsets):\n",
    "        embeds = self.embedding(texts, offsets)\n",
    "        \n",
    "        outputs = self.fc(embeds)\n",
    "        outputs = self.softmax(outputs)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def init_weights(self):\n",
    "        init_range = 0.5\n",
    "        \n",
    "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        \n",
    "        self.fc.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    labels = torch.tensor([entry[0] for entry in batch], device=device)\n",
    "    texts = [entry[1] for entry in batch]  # entry[1] are tensors. torch.tensor(tensor) is not valid\n",
    "    \n",
    "    offsets = torch.tensor([0] + [len(text) for text in texts[:-1]], device=device)\n",
    "    \n",
    "    # Concats texts.\n",
    "    texts = torch.cat(texts, dim=0).to(device)\n",
    "    # Calculates offsets.\n",
    "    offsets = offsets.cumsum(dim=0)\n",
    "\n",
    "    return texts, labels, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def _train(model, train_set, criterion, optimizer, scheduler, batch_size):\n",
    "\n",
    "    data_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, \n",
    "                     collate_fn=collate_fn)\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    for i, (texts, labels, offsets) in enumerate(data_loader):\n",
    "        # Zeros grad.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Foreward.\n",
    "        outputs = model(texts, offsets)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updates params.\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss\n",
    "        train_acc += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "        \n",
    "    # Updates lr.\n",
    "    scheduler.step()\n",
    "    \n",
    "    return train_loss / len(train_set), train_acc / len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _valid(model, valid_set, criterion, batch_size):\n",
    "    \n",
    "    data_loader = DataLoader(dataset=valid_set, batch_size=batch_size, shuffle=False, \n",
    "                     collate_fn=collate_fn)\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "    \n",
    "    for texts, labels, offsets in data_loader:\n",
    "        with torch.no_grad():  # no grad\n",
    "            # Foreward.\n",
    "            outputs = model(texts, offsets)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            valid_loss += loss\n",
    "            valid_acc += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "        \n",
    "    return valid_loss / len(valid_set), valid_acc / len(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(train_set, valid_set, model, criterion, optimizer, scheduler, n_epochs=10, batch_size=10):\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss, train_acc = _train(model, train_set, criterion, optimizer, scheduler, batch_size)\n",
    "        valid_loss, valid_acc = _valid(model, valid_set, criterion, batch_size)\n",
    "        \n",
    "        print(f\"epoch: {epoch}\")\n",
    "        print(f\"train loss: {train_loss:.3f}\\ttrain acc: {train_acc:.6%}\")\n",
    "        print(f\"valid loss: {valid_loss:.3f}\\tvalid acc: {valid_acc:.6%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_set, model, criterion, batch_size=10):\n",
    "    print(\"Testing...\")\n",
    "    \n",
    "    test_loss, test_acc = _valid(model, test_set, criterion, batch_size=batch_size)\n",
    "    \n",
    "    print(f\"test loss: {test_loss:.3f}\\ttest acc: {test_acc:.6%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:10, 11163.69lines/s]\n",
      "120000lines [00:18, 6435.63lines/s]\n",
      "7600lines [00:01, 6810.58lines/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "epoch: 1\n",
      "train loss: 0.064\ttrain acc: 73.325000%\n",
      "valid loss: 0.055\tvalid acc: 88.237500%\n",
      "epoch: 2\n",
      "train loss: 0.053\ttrain acc: 90.813542%\n",
      "valid loss: 0.053\tvalid acc: 89.829167%\n",
      "epoch: 3\n",
      "train loss: 0.051\ttrain acc: 93.593750%\n",
      "valid loss: 0.053\tvalid acc: 90.500000%\n",
      "epoch: 4\n",
      "train loss: 0.050\ttrain acc: 95.010417%\n",
      "valid loss: 0.052\tvalid acc: 90.708333%\n",
      "epoch: 5\n",
      "train loss: 0.049\ttrain acc: 95.785417%\n",
      "valid loss: 0.052\tvalid acc: 91.037500%\n",
      "epoch: 6\n",
      "train loss: 0.049\ttrain acc: 96.230208%\n",
      "valid loss: 0.052\tvalid acc: 90.950000%\n",
      "epoch: 7\n",
      "train loss: 0.049\ttrain acc: 96.536458%\n",
      "valid loss: 0.052\tvalid acc: 91.091667%\n",
      "epoch: 8\n",
      "train loss: 0.049\ttrain acc: 96.709375%\n",
      "valid loss: 0.052\tvalid acc: 91.137500%\n",
      "epoch: 9\n",
      "train loss: 0.049\ttrain acc: 96.853125%\n",
      "valid loss: 0.052\tvalid acc: 91.216667%\n",
      "epoch: 10\n",
      "train loss: 0.049\ttrain acc: 96.961458%\n",
      "valid loss: 0.052\tvalid acc: 91.225000%\n",
      "Testing...\n",
      "test loss: 0.058\ttest acc: 80.921053%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "    \n",
    "    # Gets datasets.\n",
    "    ngrams = 3\n",
    "#     train_set, val_set, test_set, vocab_size, cls_num = prepare_data(\n",
    "#         dataset_name=\"SogouNews\", dataset_tar=\".data/sogou_news_csv.tar.gz\", ngrams=ngrams)\n",
    "    train_set, val_set, test_set, vocab_size, cls_num = prepare_data(\n",
    "        dataset_tar='./.data/ag_news_csv.tar.gz',dataset_name=\"AG_NEWS\", ngrams=ngrams)\n",
    "    \n",
    "    # Model.\n",
    "    embed_dim = 30  # if it's set too big, cuda will be out of memory\n",
    "    model = Model(vocab_size, embed_dim, cls_num).to(device)\n",
    "\n",
    "    # Criterion.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = 5\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    gamma = 0.9\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "    # Training and validating.\n",
    "    batch_size = 16\n",
    "    train(train_set, val_set, model, criterion, optimizer, scheduler, batch_size=batch_size)\n",
    "\n",
    "    # Testing.\n",
    "    test(test_set, model, criterion, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
