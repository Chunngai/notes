{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "from torchtext.data import BucketIterator\n",
    "# from torchtext.datasets import IWSLT\n",
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "def prepare_data():\n",
    "    \n",
    "    # Sets up fields.\n",
    "#     src_field = Field(init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, \n",
    "#                       tokenize=\"spacy\", tokenizer_language='fr')\n",
    "#     trg_field = Field(init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, \n",
    "#                       tokenize=\"spacy\", tokenizer_language=\"en\")\n",
    "    src_field = Field(init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, \n",
    "                      tokenize=\"spacy\", tokenizer_language='de')\n",
    "    trg_field = Field(init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, \n",
    "                      tokenize=\"spacy\", tokenizer_language=\"en\")\n",
    "    \n",
    "#     # Makes splits for data.\n",
    "#     train_set, valid_set, test_set = IWSLT.splits(exts=('.fr', '.en'), \n",
    "#                                                   fields=(src_field, trg_field))\n",
    "    # Makes splits for data.\n",
    "    train_set, valid_set, test_set = Multi30k.splits(exts=('.de', '.en'), \n",
    "                                                  fields=(src_field, trg_field))\n",
    "    \n",
    "    # Builds the vocab.\n",
    "    src_field.build_vocab(train_set)\n",
    "    trg_field.build_vocab(train_set)\n",
    "    \n",
    "    # Makes iterator for splits.\n",
    "    train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "        datasets=(train_set, valid_set, test_set), batch_size=BATCH_SIZE, \n",
    "        device=DEVICE)\n",
    "\n",
    "    return src_field, trg_field, train_iter, valid_iter, test_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(src_vocab_size, ENC_EMB_DIM)\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        self.gru = nn.GRU(ENC_EMB_DIM, ENC_HID_DIM, bidirectional=True)  # ! hid dim cannot be omitted here\n",
    "        \n",
    "        self.fc = nn.Linear(ENC_HID_DIM * 2, DEC_HID_DIM)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # (in)  inputs: [seq_len, batch_size]\n",
    "        # (out) outputs: [seq_len, batch_size, enc_hid_dim * 2]\n",
    "        # (out) hidden: [batch_size, dec_hid_dim]\n",
    "        \n",
    "        # (in)  inputs: [seq_len, batch_size]\n",
    "        # (out) embedded: [seq_len, batch_size, enc_emb_dim]\n",
    "        embedded = self.dropout(\n",
    "            self.embedding(inputs))\n",
    "        \n",
    "        # (in)  embedded\n",
    "        # (out) outputs: [seq_len, batch_size, enc_hid_dim * 2]\n",
    "        # (out) hiddens: [2, batch_size, enc_hid_dim]\n",
    "        outputs, hiddens = self.gru(embedded)\n",
    "        \n",
    "        # (in)  hiddens\n",
    "        # (out) hidden: [batch_size, dec_hid_dim]\n",
    "        hidden = torch.tanh(  # ! tanh\n",
    "            self.fc(\n",
    "                torch.cat((hiddens[0], hiddens[1]), dim=1)))\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(DEC_HID_DIM + ENC_HID_DIM * 2, ATTN_V_DIM)\n",
    "        self.v = nn.Parameter(torch.rand(1, ATTN_V_DIM))\n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # (in)  decoder_hidden: [batch_size, dec_hid_dim]\n",
    "        # (in)  encoder_outputs: [seq_len, batch_size, enc_hid_dim * 2]\n",
    "        #!(out) context: [batch, enc_hid_dim * 2]\n",
    "        \n",
    "        # (in)  decoder_hidden\n",
    "        # (in)  encoder_outputs\n",
    "        # (out) energy: [batch_size, seq_len, attn_v_dim]\n",
    "        energy = torch.tanh(\n",
    "            self.fc(\n",
    "                torch.cat((\n",
    "                    decoder_hidden.unsqueeze(1).repeat(1, encoder_outputs.size()[0], 1), \n",
    "                    encoder_outputs.permute(1, 0, 2)), dim=2)))  # ! repeat(encoder_outputs.size()[0]) is wrong\n",
    "        \n",
    "        # (in)  v: [1, attn_v_dim]\n",
    "        # (in)  energy\n",
    "        # (out) [batch_size, 1, seq_len]\n",
    "        attn = F.softmax(self.v.unsqueeze(0).repeat(energy.size()[0], 1, 1).bmm(energy.permute(0, 2, 1)), dim=2)  # ! energy.size()[0] may not always be equal to BATCH_SIZE\n",
    "        \n",
    "        # (in)  attn\n",
    "        # (in)  encoder_outputs\n",
    "        # (out) context: [batch, enc_hid_dim]\n",
    "        context = attn.bmm(encoder_outputs.permute(1, 0, 2)).squeeze()\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, trg_vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(trg_vocab_size, DEC_EMB_DIM)\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        self.attn = Attn()\n",
    "        \n",
    "        self.gru = nn.GRU(DEC_EMB_DIM + ENC_HID_DIM * 2, DEC_HID_DIM)\n",
    "        \n",
    "        self.fc = nn.Linear(DEC_EMB_DIM + ENC_HID_DIM * 2 + DEC_HID_DIM, trg_vocab_size)\n",
    "        \n",
    "    def forward(self, last_output, decoder_hidden, encoder_outputs):\n",
    "        # (in)  last_output: [batch_size]\n",
    "        # (in)  decoder_hidden: [batch_size, dec_hid_dim]\n",
    "        # (in)  encoder_outputs: [seq_len, batch_size, enc_hid_dim * 2]\n",
    "        #!(out) decoder_outputs: [batch_size, trg_vocab_size]\n",
    "        # (out) decoder_hidden: [batch_size, dec_hid_dim]\n",
    "        \n",
    "        # (in)  last_output\n",
    "        # (out) embedded: [batch_size, dec_emb_dim]\n",
    "        embedded = self.dropout(\n",
    "            self.embedding(last_output))\n",
    "        \n",
    "        # (in)  decoder_hidden\n",
    "        # (in)  encoder_outputs\n",
    "        #!(out) context: [batch, enc_hid_dim * 2]\n",
    "        context = self.attn(decoder_hidden, encoder_outputs)\n",
    "        \n",
    "        # (in)  embedded\n",
    "        # (in)  context\n",
    "        # (in)  decoder_hidden\n",
    "        # (out) outputs: [1, batch_size, dec_hid_dim]\n",
    "        # (out) decoder_hidden: [1, batch_size, dec_hid_dim]\n",
    "        outputs, decoder_hidden = self.gru(\n",
    "            torch.cat((embedded.unsqueeze(0), context.unsqueeze(0)), dim=2), \n",
    "            decoder_hidden.unsqueeze(0))\n",
    "        \n",
    "        # (in)  embedded\n",
    "        # (in)  context\n",
    "        # (in)  decoder_hidden\n",
    "        # (out) decoder_outputs: [batch_size, trg_vocab_size]\n",
    "        decoder_outputs = self.fc(\n",
    "            torch.cat((embedded, context, decoder_hidden.squeeze(0)), dim=1))\n",
    "        \n",
    "        return decoder_outputs, decoder_hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(src_vocab_size)\n",
    "        self.decoder = Decoder(trg_vocab_size)\n",
    "        \n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        \n",
    "    def forward(self, inputs, trgs, teacher_forcing_ratio=0.5):\n",
    "        # (in)  inputs: [src_len, batch_size]\n",
    "        # (in)  trgs: [trg_len, batch_size]\n",
    "        # (out) outputs: [trg_len, batch_size, trg_vocab_size]\n",
    "        \n",
    "        # seq len of inputs and trgs may not always be the same\n",
    "                \n",
    "        # Encode.\n",
    "        # (in)  inputs\n",
    "        # (out) encoder_outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        # (out) decoder_hidden: [batch_size, dec_hid_dim]\n",
    "        encoder_outputs, decoder_hidden = self.encoder(inputs)\n",
    "        \n",
    "        # Decode.\n",
    "        trg_len = trgs.size()[0]\n",
    "        batch_size = trgs.size()[1]\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, self.trg_vocab_size, device=DEVICE)  # batch_size may not always be equal to BATCH_SIZE\n",
    "\n",
    "        decoder_outputs = trgs[0]\n",
    "        for t in range(1, trg_len):  # counts from 1\n",
    "            # (in)  decoder_output: [batch_size]\n",
    "            # (in)  decoder_hidden\n",
    "            # (in)  encoder_outputs\n",
    "            #!(out) decoder_outputs: [batch_size, trg_vocab_size]\n",
    "            # (out) decoder_hidden: [batch_size, dec_hid_dim]\n",
    "            decoder_outputs, decoder_hidden = self.decoder(decoder_outputs, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            outputs[t] = decoder_outputs\n",
    "            \n",
    "            decoder_outputs = decoder_outputs.argmax(dim=1) if teacher_forcing_ratio <= random.random() else trgs[t]\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train(train_iter, model, criterion, optimizer):\n",
    "    \n",
    "    train_loss = 0\n",
    "    \n",
    "    model.train()  # !\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        \n",
    "        # Gets data.\n",
    "        srcs = batch.src\n",
    "        trgs = batch.trg\n",
    "        \n",
    "        # Forward.\n",
    "        outputs = model(srcs, trgs)\n",
    "        \n",
    "        # Loss.\n",
    "        trgs = trgs[1:].view(-1)  # ! [1:]\n",
    "        outputs = outputs[1:].view(-1, outputs.size()[-1])  # ! [1:]\n",
    "        loss = criterion(outputs, trgs)\n",
    "        \n",
    "        # Backward.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updates params\n",
    "        optimizer.step()\n",
    "        # Zeros grad.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    return train_loss / len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(data_iter, model, criterion):\n",
    "    \n",
    "    eval_loss = 0\n",
    "    \n",
    "    model.eval()  # !\n",
    "    \n",
    "    with torch.no_grad():  # !\n",
    "        for batch in data_iter:\n",
    "\n",
    "            # Gets data.\n",
    "            srcs = batch.src\n",
    "            trgs = batch.trg\n",
    "\n",
    "            # Forward.\n",
    "            outputs = model(srcs, trgs, 0)\n",
    "\n",
    "            # Loss.\n",
    "            trgs = trgs[1:].view(-1)  # ! [1:]\n",
    "            outputs = outputs[1:].view(-1, outputs.size()[-1])  # [1:]\n",
    "            loss = criterion(outputs, trgs)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "        \n",
    "        return eval_loss / len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_track(start, end):\n",
    "    \n",
    "    elapsed_time = end - start\n",
    "    \n",
    "    mins = int(elapsed_time / 60)\n",
    "    secs = int(elapsed_time % 60)\n",
    "    \n",
    "    return f\"{mins:>2}mins {secs:>2}secs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def train(train_iter, valid_iter, model, criterion, optimizer):\n",
    "        \n",
    "    for epoch in range(N_EPOCHS):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        train_loss = _train(train_iter, model, criterion, optimizer)\n",
    "        valid_loss = _evaluate(valid_iter, model, criterion)\n",
    "    \n",
    "        end = time.time()\n",
    "        \n",
    "        print(f\"epoch: {epoch + 1:02}, time: {time_track(start, end)}\")\n",
    "        print(f\"train loss: {train_loss:.3f}, train ppl: {math.exp(train_loss):.3f}\")\n",
    "        print(f\"valid loss: {valid_loss:.3f}, valid ppl: {math.exp(valid_loss):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_iter, model, criterion):\n",
    "    \n",
    "    test_loss = _evaluate(test_iter, model, criterion)\n",
    "\n",
    "    print(f\"test loss: {test_loss:.3f}, test ppl: {math.exp(test_loss):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, time: 22mins 38secs\n",
      "train loss: 5.348, train ppl: 210.142\n",
      "valid loss: 4.922, valid ppl: 137.286\n",
      "epoch: 02, time: 22mins  2secs\n",
      "train loss: 3.919, train ppl: 50.375\n",
      "valid loss: 3.985, valid ppl: 53.801\n",
      "epoch: 03, time: 21mins 45secs\n",
      "train loss: 3.144, train ppl: 23.201\n",
      "valid loss: 3.613, valid ppl: 37.081\n",
      "epoch: 04, time: 21mins 54secs\n",
      "train loss: 2.704, train ppl: 14.940\n",
      "valid loss: 3.581, valid ppl: 35.902\n",
      "epoch: 05, time: 21mins 21secs\n",
      "train loss: 2.368, train ppl: 10.680\n",
      "valid loss: 3.602, valid ppl: 36.679\n",
      "epoch: 06, time: 21mins 39secs\n",
      "train loss: 2.220, train ppl: 9.209\n",
      "valid loss: 3.562, valid ppl: 35.220\n",
      "epoch: 07, time: 28mins 39secs\n",
      "train loss: 2.055, train ppl: 7.808\n",
      "valid loss: 3.700, valid ppl: 40.440\n",
      "epoch: 08, time: 23mins 57secs\n",
      "train loss: 1.971, train ppl: 7.181\n",
      "valid loss: 3.721, valid ppl: 41.323\n",
      "epoch: 09, time: 22mins 47secs\n",
      "train loss: 1.868, train ppl: 6.474\n",
      "valid loss: 3.718, valid ppl: 41.199\n",
      "epoch: 10, time: 22mins 25secs\n",
      "train loss: 1.796, train ppl: 6.026\n",
      "valid loss: 3.805, valid ppl: 44.927\n",
      "test loss: 3.828, test ppl: 45.959\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    BATCH_SIZE = 512\n",
    "#     DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "    N_EPOCHS = 10\n",
    "\n",
    "    ENC_EMB_DIM = 256\n",
    "    ENC_HID_DIM = 512\n",
    "\n",
    "    ATTN_V_DIM = 64\n",
    "\n",
    "    DEC_EMB_DIM = 256\n",
    "    DEC_HID_DIM = 512\n",
    "\n",
    "    # Gets data.\n",
    "    src_field, trg_field, train_iter, valid_iter, test_iter = prepare_data()\n",
    "        \n",
    "    # Gets a model instance.\n",
    "    src_vocab_size = len(src_field.vocab)\n",
    "    trg_vocab_size = len(trg_field.vocab)\n",
    "    model = Seq2Seq(src_vocab_size, trg_vocab_size).to(DEVICE)  # ! to(DEVICE)\n",
    "\n",
    "    # Criterion.\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=trg_field.vocab.stoi['<pad>'])\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "    # Trains and validates the model.\n",
    "    train(train_iter, valid_iter, model, criterion, optimizer)\n",
    "\n",
    "    # Tests the model.\n",
    "    test(test_iter, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
