{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import TranslationDataset\n",
    "\n",
    "class GEC_DATASET(TranslationDataset):\n",
    "    @classmethod\n",
    "    def splits(cls, \n",
    "               exts, \n",
    "               fields, \n",
    "               root, \n",
    "               train=\"train\", \n",
    "               validation=\"dev\", \n",
    "               test=\"test\", \n",
    "               **kwargs):\n",
    "        \n",
    "        return super(GEC_DATASET, cls).splits(exts=exts, \n",
    "                                      fields=fields,\n",
    "                                      path=root, \n",
    "                                      root=root,\n",
    "                                      train=train, \n",
    "                                      validation=validation, \n",
    "                                      test=test,\n",
    "                                      **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "def prepare_data(root):\n",
    "    src_field = Field(init_token=\"<sos>\", eos_token=\"<eos>\", tokenize=lambda sentence: sentence.split(' '))\n",
    "    trg_field = Field(init_token=\"<sos>\", eos_token=\"<eos>\", tokenize=lambda sentence: sentence.split(' '))\n",
    "    \n",
    "    #? Should be lower?\n",
    "    \n",
    "    train_set, valid_set, test_set = GEC_DATASET.splits(exts=('.src', '.trg'), \n",
    "                                                        fields=(src_field, trg_field),\n",
    "                                                        root=root,\n",
    "                                                        filter_pred=lambda sentence: \n",
    "                                                        len(vars(sentence)['src']) < MAX_LEN \n",
    "                                                        and len(vars(sentence)['trg']) < MAX_LEN)\n",
    "\n",
    "    src_field.build_vocab(train_set, min_freq=MIN_FREQ)\n",
    "    trg_field.build_vocab(train_set, min_freq=MIN_FREQ)\n",
    "\n",
    "    train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "        datasets=(train_set, valid_set, test_set), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        device=DEVICE)\n",
    "    \n",
    "    return src_field, trg_field, train_iter, valid_iter, test_iter, train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size):  \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(src_vocab_size, ENC_EMB_DIM)\n",
    "        self.dropout = nn.Dropout(ENC_DROPOUT)  #+ dropout\n",
    "        \n",
    "        self.gru = nn.GRU(ENC_EMB_DIM, ENC_HID_DIM, bidirectional=True)\n",
    "        \n",
    "        self.fc = nn.Linear(ENC_HID_DIM * 2, DEC_HID_DIM)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # (in)  inputs: [src_len, batch_size]\n",
    "        # (out) outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        # (out) hidden: [batch_size, dec_hid_dim]\n",
    "        \n",
    "        # (in)  inputs\n",
    "        # (out) embedded: [src_len, batch_size, enc_emb_dim]\n",
    "        embedded = self.dropout(\n",
    "            self.embedding(inputs))\n",
    "        \n",
    "        # (in)  embedded\n",
    "        # (out) outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        # (out) hiddens: [2, batch_size, enc_hid_dim]\n",
    "        outputs, hiddens = self.gru(embedded)\n",
    "        \n",
    "        # (in)  hiddens\n",
    "        # (out) hidden: [batch_size, dec_hid_dim]\n",
    "        hidden = torch.tanh(\n",
    "            self.fc(\n",
    "                torch.cat((hiddens[0], \n",
    "                           hiddens[1]), dim=1)))\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(DEC_HID_DIM + ENC_HID_DIM * 2, ATTN_V_DIM)\n",
    "        self.v = nn.Parameter(torch.rand(1, ATTN_V_DIM))\n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # (in)  decoder_hidden: [batch_size, dec_hid_dim]\n",
    "        # (in)  encoder_outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        # (out) attn: [batch_size, src_len]\n",
    "        \n",
    "        # (in)  decoder_hidden\n",
    "        # (in)  encoder_outputs\n",
    "        # (out) energy: [batch_size, src_len, attn_v_dim]\n",
    "        energy = torch.tanh(\n",
    "            self.fc(\n",
    "                torch.cat((\n",
    "                    decoder_hidden.unsqueeze(1).repeat(1, encoder_outputs.size()[0], 1), \n",
    "                    encoder_outputs.permute(1, 0, 2)), dim=2)))\n",
    "        \n",
    "        # (in)  v: [1, attn_v_dim]\n",
    "        # (in)  energy\n",
    "        # (out) attn: [batch_size, src_len]\n",
    "        attn = F.softmax(self.v.unsqueeze(0).repeat(energy.size()[0], 1, 1).bmm(energy.permute(0, 2, 1)), dim=2).squeeze(1)  #m\n",
    "        \n",
    "        return attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, trg_vocab_size):  \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(trg_vocab_size, DEC_EMB_DIM)\n",
    "        self.dropout = nn.Dropout(DEC_DROPOUT)  #+ dropout\n",
    "        \n",
    "        self.attn = Attn()\n",
    "        \n",
    "        self.gru = nn.GRU(DEC_EMB_DIM + ENC_HID_DIM * 2, DEC_HID_DIM)\n",
    "        \n",
    "        self.fc = nn.Linear(DEC_EMB_DIM + ENC_HID_DIM * 2 + DEC_HID_DIM, trg_vocab_size)\n",
    "        \n",
    "    def forward(self, last_output, decoder_hidden, encoder_outputs):\n",
    "        # (in)  last_output: [batch_size]\n",
    "        # (in)  decoder_hidden: [batch_size, dec_hid_dim]\n",
    "        # (in)  encoder_outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        #!(out) decoder_outputs: [batch_size, trg_vocab_size]\n",
    "        # (out) decoder_hidden: [batch_size, dec_hid_dim]\n",
    "        \n",
    "        # (in)  last_output\n",
    "        # (out) embedded: [batch_size, dec_emb_dim]\n",
    "        embedded = self.dropout(\n",
    "            self.embedding(last_output))\n",
    "        \n",
    "        # (in)  decoder_hidden\n",
    "        # (in)  encoder_outputs\n",
    "        # (out) attn: [batch_size, src_len]\n",
    "        attn = self.attn(decoder_hidden, encoder_outputs)\n",
    "        # (in)  attn\n",
    "        # (in)  encoder_outputs\n",
    "        # (out) context: [batch, enc_hid_dim * 2]\n",
    "        context = attn.unsqueeze(1).bmm(encoder_outputs.permute(1, 0, 2)).squeeze(1)\n",
    "\n",
    "        # (in)  embedded\n",
    "        # (in)  context\n",
    "        # (in)  decoder_hidden\n",
    "        # (out) outputs: [1, batch_size, dec_hid_dim]\n",
    "        # (out) decoder_hidden: [1, batch_size, dec_hid_dim]\n",
    "        outputs, decoder_hidden = self.gru(\n",
    "            torch.cat((embedded.unsqueeze(0), \n",
    "                       context.unsqueeze(0)), dim=2), \n",
    "            decoder_hidden.unsqueeze(0))\n",
    "        \n",
    "        # (in)  embedded\n",
    "        # (in)  context\n",
    "        # (in)  decoder_hidden\n",
    "        # (out) decoder_outputs: [batch_size, trg_vocab_size]\n",
    "        decoder_outputs = self.fc(\n",
    "            torch.cat((embedded, \n",
    "                       context, \n",
    "                       decoder_hidden.squeeze(0)), dim=1))\n",
    "        \n",
    "        return decoder_outputs, decoder_hidden.squeeze(0), attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(src_vocab_size)\n",
    "        self.decoder = Decoder(trg_vocab_size)\n",
    "        \n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        \n",
    "    def forward(self, inputs, trgs, teacher_forcing_ratio=0.5):\n",
    "        # (in)  inputs: [src_len, batch_size]\n",
    "        # (in)  trgs: [trg_len, batch_size]\n",
    "        # (out) outputs: [trg_len, batch_size, trg_vocab_size]\n",
    "        \n",
    "        # seq len of inputs and trgs may not always be the same\n",
    "                \n",
    "        # Encode.\n",
    "        # (in)  inputs\n",
    "        # (out) encoder_outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        # (out) decoder_hidden: [batch_size, dec_hid_dim]\n",
    "        encoder_outputs, decoder_hidden = self.encoder(inputs)\n",
    "        \n",
    "        # Decode.\n",
    "        trg_len = trgs.size()[0]\n",
    "        batch_size = trgs.size()[1]\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, self.trg_vocab_size, device=DEVICE)\n",
    "\n",
    "        decoder_outputs = trgs[0]\n",
    "        for t in range(1, trg_len):\n",
    "            # (in)  decoder_output: [batch_size]\n",
    "            # (in)  decoder_hidden\n",
    "            # (in)  encoder_outputs\n",
    "            #!(out) decoder_outputs: [batch_size, trg_vocab_size]\n",
    "            # (out) decoder_hidden: [batch_size, dec_hid_dim]\n",
    "            decoder_outputs, decoder_hidden, _ = self.decoder(decoder_outputs, \n",
    "                                                           decoder_hidden, \n",
    "                                                           encoder_outputs)\n",
    "            \n",
    "            outputs[t] = decoder_outputs\n",
    "            \n",
    "            decoder_outputs = decoder_outputs.argmax(dim=1) if teacher_forcing_ratio <= random.random() else trgs[t]\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):  #+ init weights\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train(train_iter, model, criterion, optimizer):\n",
    "    \n",
    "    train_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        \n",
    "        # Gets data.\n",
    "        srcs = batch.src\n",
    "        trgs = batch.trg\n",
    "        \n",
    "        # Forward.\n",
    "        outputs = model(srcs, trgs)\n",
    "        \n",
    "        # Loss.\n",
    "        loss = criterion(outputs[1:].view(-1, outputs.size()[-1]), \n",
    "                         trgs[1:].view(-1))  #m\n",
    "        \n",
    "        # Backward.\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)  #+\n",
    "        \n",
    "        # Updates params\n",
    "        optimizer.step()\n",
    "        # Zeros grad.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    return train_loss / len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(data_iter, model, criterion):\n",
    "    \n",
    "    eval_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter:\n",
    "\n",
    "            # Gets data.\n",
    "            srcs = batch.src\n",
    "            trgs = batch.trg\n",
    "\n",
    "            # Forward.\n",
    "            outputs = model(srcs, trgs, 0)\n",
    "\n",
    "            # Loss.\n",
    "            loss = criterion(outputs[1:].view(-1, outputs.size()[-1]), \n",
    "                             trgs[1:].view(-1))  #m\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "        \n",
    "        return eval_loss / len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_track(start, end):\n",
    "    \n",
    "    elapsed_time = end - start\n",
    "    \n",
    "    mins = int(elapsed_time / 60)\n",
    "    secs = int(elapsed_time % 60)\n",
    "    \n",
    "    return f\"{mins:>2}mins {secs:>2}secs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import copy\n",
    "\n",
    "def train(train_iter, valid_iter, model, criterion, optimizer):\n",
    "        \n",
    "    min_valid_loss = float(\"inf\")  #+\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        train_loss = _train(train_iter, model, criterion, optimizer)\n",
    "        valid_loss = _evaluate(valid_iter, model, criterion)\n",
    "    \n",
    "        end = time.time()\n",
    "        \n",
    "        print(f\"epoch: {epoch + 1:02}, time: {time_track(start, end)}\")\n",
    "        print(f\"train loss: {train_loss:.3f}, train ppl: {math.exp(train_loss):.3f}\")\n",
    "        print(f\"valid loss: {valid_loss:.3f}, valid ppl: {math.exp(valid_loss):.3f}\")\n",
    "        \n",
    "        if valid_loss < min_valid_loss:  #+\n",
    "            min_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), PT)\n",
    "            \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_iter, model, criterion):\n",
    "    model.load_state_dict(torch.load(PT))\n",
    "    \n",
    "    test_loss = _evaluate(test_iter, model, criterion)\n",
    "\n",
    "    print(f\"test loss: {test_loss:.3f}, test ppl: {math.exp(test_loss):.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference  #+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sentence(sentence, src_field, trg_field, model):\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        tokens = [token for token in sentence.split(' ')]\n",
    "    else:\n",
    "        tokens = sentence\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.tensor(src_indexes, dtype=torch.long, device=DEVICE).unsqueeze(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "        \n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "    attentions = torch.zeros(MAX_LEN, 1, len(src_indexes), device=DEVICE)\n",
    "    \n",
    "    for i in range(MAX_LEN):\n",
    "\n",
    "        trg_tensor = torch.tensor([trg_indexes[-1]], dtype=torch.long, device=DEVICE)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
    "            \n",
    "        pred_token = output.argmax(1).item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "        attentions[i] = attention\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, correction, attention):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def inference(dataset, src_field, trg_field, model):\n",
    "    \n",
    "    for _ in range(10):\n",
    "        example_idx = random.randint(1, len(dataset))\n",
    "        \n",
    "        src = vars(dataset.examples[example_idx])['src']\n",
    "        trg = vars(dataset.examples[example_idx])['trg']\n",
    "\n",
    "        print(f\"src = {' '.join(src)}\")\n",
    "        print(f\"trg = {' '.join(trg)}\")\n",
    "        \n",
    "        correction, attention = correct_sentence(src, src_field, trg_field, model)\n",
    "        \n",
    "        print(f\"out = {' '.join(correction[:-1])}\")\n",
    "        \n",
    "        print('---')\n",
    "        \n",
    "#         display_attention(src, correction, attention)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU #+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model):\n",
    "    \n",
    "    trgs = []\n",
    "    outs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        \n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        \n",
    "        out, _ = correct_sentence(src, src_field, trg_field, model)\n",
    "        \n",
    "        #cut off <eos> token\n",
    "        out = out[:-1]\n",
    "        \n",
    "        outs.append(out)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "    return bleu_score(outs, trgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, time:  5mins 11secs\n",
      "train loss: 5.200, train ppl: 181.214\n",
      "valid loss: 4.254, valid ppl: 70.353\n",
      "epoch: 02, time:  5mins 12secs\n",
      "train loss: 2.970, train ppl: 19.493\n",
      "valid loss: 2.795, valid ppl: 16.369\n",
      "epoch: 03, time:  5mins 15secs\n",
      "train loss: 1.912, train ppl: 6.766\n",
      "valid loss: 2.545, valid ppl: 12.740\n",
      "epoch: 04, time:  5mins 14secs\n",
      "train loss: 1.458, train ppl: 4.295\n",
      "valid loss: 2.286, valid ppl: 9.839\n",
      "epoch: 05, time:  5mins 12secs\n",
      "train loss: 1.158, train ppl: 3.183\n",
      "valid loss: 2.296, valid ppl: 9.939\n",
      "epoch: 06, time:  5mins 12secs\n",
      "train loss: 0.958, train ppl: 2.606\n",
      "valid loss: 2.254, valid ppl: 9.527\n",
      "epoch: 07, time:  5mins 13secs\n",
      "train loss: 0.802, train ppl: 2.231\n",
      "valid loss: 2.264, valid ppl: 9.623\n",
      "epoch: 08, time:  5mins 11secs\n",
      "train loss: 0.683, train ppl: 1.980\n",
      "valid loss: 2.336, valid ppl: 10.341\n",
      "epoch: 09, time:  5mins 13secs\n",
      "train loss: 0.590, train ppl: 1.805\n",
      "valid loss: 2.361, valid ppl: 10.599\n",
      "epoch: 10, time:  5mins 14secs\n",
      "train loss: 0.506, train ppl: 1.659\n",
      "valid loss: 2.396, valid ppl: 10.980\n",
      "\n",
      "test loss: 2.982, test ppl: 19.734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    DATASET = \"../data/fce with bpe\"\n",
    "    \n",
    "    # Prepares for data.\n",
    "    ROOT = f\"{DATASET}/parallel\"\n",
    "    MAX_LEN = 100\n",
    "    MIN_FREQ = 1\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    src_field, trg_field, train_iter, valid_iter, test_iter, train_set, valid_set, test_set = prepare_data(root=ROOT)\n",
    "    \n",
    "    # Model.\n",
    "    ENC_EMB_DIM = 600\n",
    "    ENC_HID_DIM = 1_000\n",
    "    ENC_DROPOUT = 0.5\n",
    "\n",
    "    DEC_EMB_DIM = 600\n",
    "    DEC_HID_DIM = 1_000\n",
    "    DEC_DROPOUT = 0.5\n",
    "    \n",
    "    ATTN_V_DIM = 1_000  #m\n",
    "    \n",
    "    model = Seq2Seq(len(src_field.vocab), len(trg_field.vocab)).to(DEVICE)\n",
    "    model.apply(init_weights)\n",
    "    \n",
    "    # Criterion.\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=trg_field.vocab.stoi[\"<pad>\"])\n",
    "    \n",
    "    # Optimizer.\n",
    "    LR = 0.0003\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)  #n Decreasing lr makes the change of valid_loss slowlier, thus easier to get the optimal?\n",
    "    \n",
    "    # Trains and validates.\n",
    "    N_EPOCHS = 10\n",
    "    CLIP = 1  #+\n",
    "    PT = f\"{DATASET}.pt\"\n",
    "    \n",
    "    train(train_iter, valid_iter, model, criterion, optimizer)\n",
    "    \n",
    "    # Tests.\n",
    "    test(test_iter, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training inference\n",
      "src = I can say that I want to go .\n",
      "trg = I can say that I wanted to go ,\n",
      "out = I can say that I want to go .\n",
      "---\n",
      "src = If you have something to ask , please call me .\n",
      "trg = If you have something to ask , please call me .\n",
      "out = If you have something to ask , please call me .\n",
      "---\n",
      "src = 0@@ 9@@ 1@@ 11 Che@@ m@@ ni@@ k\n",
      "trg = 0@@ 9@@ 1@@ 11 Che@@ m@@ ni@@ k\n",
      "out = 0@@ 9@@ 1@@ 11 Che@@ m@@ ni@@ k\n",
      "---\n",
      "src = I am writing a letter to complain about the musical show I saw last week when I was in London .\n",
      "trg = I am writing a letter to complain about the musical show I saw last week when I was in London .\n",
      "out = I am writing a letter to complain about the musical show I saw last week when I was in London .\n",
      "---\n",
      "src = The aim of this report is about suggesting which lessons and other activities should be filmed and why are suitable to make a film .\n",
      "trg = The aim of this report is to suggest which lessons and other activities should be filmed and why they are suitable to make a film of .\n",
      "out = The aim of this report is about the which lessons and other activities should be filmed and why are suitable for make a film .\n",
      "---\n",
      "src = I am so happy to win the prize and come true my dream .\n",
      "trg = I am so happy to win the prize and realise my dream .\n",
      "out = I am so happy to win the prize and come true my dream .\n",
      "---\n",
      "src = Mrs Ryan ,\n",
      "trg = Mrs Ryan ,\n",
      "out = Mrs Ryan ,\n",
      "---\n",
      "src = That is a sport that I always want to practise but in France it is to exp@@ an@@ sive .\n",
      "trg = This is a sport that I have always wanted to play but in France it is too expensive .\n",
      "out = That is a sport that I always want to do but in France it is to exp@@ .\n",
      "---\n",
      "src = As you know , I 'm crazy about pop music .\n",
      "trg = As you know , I 'm crazy about pop music .\n",
      "out = As you know , I 'm crazy about pop music .\n",
      "---\n",
      "src = It broke all my impression about London .\n",
      "trg = It spoilt all my impression of London .\n",
      "out = It spoilt all my impression about London .\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"training inference\")\n",
    "inference(train_set, src_field, trg_field, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating inference\n",
      "src = Do you have any idea what is the weather going to be ?\n",
      "trg = Do you have any idea what the weather is going to be like ?\n",
      "out = Do you have any idea what is the weather going to be ?\n",
      "---\n",
      "src = Dear Sir !\n",
      "trg = Dear Sir !\n",
      "out = Dear Sir !\n",
      "---\n",
      "src = In addition to this we use the net to comunicate with some friends we have in U.S.A. It 's really amazing .\n",
      "trg = In addition to this we use the net to communicate with some friends we have in the U.S.A. It 's really amazing .\n",
      "out = In addition to this we use the net to communicate with some friends we have in the advertisement 's amazing .\n",
      "---\n",
      "src = Dear Mr Robertson !\n",
      "trg = Dear Mr Robertson ,\n",
      "out = Dear Mr Robertson !\n",
      "---\n",
      "src = People in general have a compul@@ sory acti@@ tu@@ d for shopping in my opinion .\n",
      "trg = People in general have a compul@@ sion to shop , in my opinion .\n",
      "out = People in general have a questionnaire ant for shopping in my opinion .\n",
      "---\n",
      "src = Moreover there was no discount ticket .\n",
      "trg = Moreover there were no discount tickets .\n",
      "out = Moreover there was no discount ticket .\n",
      "---\n",
      "src = I also choosed painting because I have never done before , I would like to learn and moreover I want to do a calm activity .\n",
      "trg = I also chose painting because I have never done it before . I would like to learn and moreover I want to do a relaxing activity .\n",
      "out = I also chosen painting because I have never done before , I would like to learn and I want to do a a activity activity .\n",
      "---\n",
      "src = Organ@@ ize better your shows and do not exag@@ er@@ ate with publicity !\n",
      "trg = Or@@ gan@@ ise your shows better and do not exag@@ ger@@ ate with publicity !\n",
      "out = Hu@@ li@@ better your shows and do not m@@ er@@ ate with fridges !\n",
      "---\n",
      "src = Yours sincerely ,\n",
      "trg = Yours sincerely ,\n",
      "out = Yours sincerely ,\n",
      "---\n",
      "src = This holiday will help me to think and prepare it .\n",
      "trg = This holiday will help me to think about it and prepare it .\n",
      "out = This holiday will help me to think and prepare it .\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"validating inference\")\n",
    "inference(valid_set, src_field, trg_field, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing inference\n",
      "src = I want to inform you that I had already talked to Ric@@ hard Brown and he informed me about the conf@@ er@@ ence .\n",
      "trg = I want to inform you that I have already talked to Ric@@ hard Brown and he informed me about the conf@@ er@@ ence .\n",
      "out = I want to inform you that I had already talked to feel hard and and told me about the hy@@ er@@ ent .\n",
      "---\n",
      "src = Some people might say the electricity more important than another thin@@ gh .\n",
      "trg = Some people might say electricity is more important than any other thing .\n",
      "out = Some people might say the politicians more important than another films .\n",
      "---\n",
      "src = I am looking forward to seeing you at the conf@@ er@@ ence .\n",
      "trg = I am looking forward to seeing you at the conf@@ er@@ ence .\n",
      "out = I am looking forward to seeing you at the Aud@@ er@@ .\n",
      "---\n",
      "src = After G@@ a@@ udi death it was critical stop which was fortunately solved three years later .\n",
      "trg = After G@@ a@@ udi 's death it was critical stop which was fortunately solved three years later .\n",
      "out = After G@@ a@@ ump@@ death it was avoi@@ ding which was brilliant lasted three years later .\n",
      "---\n",
      "src = Let me give you some further information concerning the event .\n",
      "trg = Let me give you some further information concerning the event .\n",
      "out = Let me give you some further information concerning the event .\n",
      "---\n",
      "src = I 'd suggest you to wear at least , se@@ m@@ i formal wears such as a jacket with another clothes .\n",
      "trg = I 'd suggest you wear at least , se@@ m@@ i - formal clothes such as a jacket with other clothes .\n",
      "out = I 'd suggest you wear at least , , so I can a a with with the clothes .\n",
      "---\n",
      "src = The best way to get there , is to take the bus number 7 .\n",
      "trg = The best way to get there is to take the number 7 bus .\n",
      "out = The best way to get there , is to take the bus trend 7 .\n",
      "---\n",
      "src = I am very happy to help you and I hope that the informations , that I am going to give you , will be enough .\n",
      "trg = I am very happy to help you and I hope that the information that I am going to give you will be satisfactory .\n",
      "out = I am very happy to help you and I hope that the information that I am going to give you , will be enough .\n",
      "---\n",
      "src = I would suggest everybody to wear a casual dresses .\n",
      "trg = I would suggest everybody wear casual dress .\n",
      "out = I would suggest everybody to wear a formal dresses .\n",
      "---\n",
      "src = As you know , we 've been working very hard all this term .\n",
      "trg = As you know , we 've been working very hard all this term .\n",
      "out = As you know , we 've been very hard hard hard all this term .\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"testing inference\")\n",
    "inference(test_set, src_field, trg_field, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 54.54\n"
     ]
    }
   ],
   "source": [
    "bleu = calculate_bleu(test_set, src_field, trg_field, model)\n",
    "print(f'BLEU score = {bleu*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
