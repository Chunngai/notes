{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang():\n",
    "    def __init__(self, lang_name):\n",
    "        self.name = lang_name\n",
    "        \n",
    "        self.word2index = {}\n",
    "        self.index2word = {0: \"<SOS>\", 1: \"<EOS>\"}\n",
    "        \n",
    "        self.word_count = {}\n",
    "        \n",
    "        self.n_words = 2\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word_count:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            \n",
    "            self.word_count[word] = 1\n",
    "            \n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word_count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def unicode_2_ascii(sentence):\n",
    "    ascii_str = ''\n",
    "    \n",
    "    for char in unicodedata.normalize(\"NFD\", sentence):\n",
    "        if unicodedata.category(char) != \"Mn\":\n",
    "            ascii_str += char\n",
    "            \n",
    "    return ascii_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_string(sentence):\n",
    "    # Lowercase.\n",
    "    sentence = sentence.lower()\n",
    "    # Unicode -> ascii.\n",
    "    sentence = unicode_2_ascii(sentence)\n",
    "    # Adds a space between chrs and [.!?].\n",
    "    sentence = re.sub(r\"([.!?])\", r\" \\1\", sentence)  # r''\n",
    "    # Keeps only letters and [.!?].\n",
    "    sentence = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pairs(file):\n",
    "    with open(file, encoding=\"utf-8\") as f:\n",
    "        file_data = f.read()\n",
    "        \n",
    "    # Puts lines in a list.\n",
    "    lines = file_data.strip().split('\\n')\n",
    "    # Splits into pairs.\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "    # Strips sentences.\n",
    "    pairs = [[sentence.strip() for sentence in pair] for pair in pairs]  # two []s\n",
    "    # Normalizes sentences.\n",
    "    pairs = [[normalize_string(sentence) for sentence in pair] for pair in pairs]\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filter_(pairs, max_len):\n",
    "    return [pair for pair in pairs if len(pair[0].split(' ')) < max_len \n",
    "            and len(pair[1].split(' ')) < max_len\n",
    "            and pair[0].startswith(eng_prefixes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(lang1, lang2, reverse=False, max_len=10):\n",
    "    print(\"preparing data...\")\n",
    "    \n",
    "    # Generates the file name.\n",
    "    file = f\"data/{lang1}-{lang2}.txt\"\n",
    "    # Reads sentence pairs from file.\n",
    "    pairs = read_pairs(file)\n",
    "    # Filters with max len.\n",
    "    pairs = filter_(pairs, max_len)\n",
    "    \n",
    "    # Creates Lang objs.\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(pair)) for pair in pairs]\n",
    "        \n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        \n",
    "    for input_sentence, output_sentence in pairs:\n",
    "        input_lang.add_sentence(input_sentence)\n",
    "        output_lang.add_sentence(output_sentence)\n",
    "    \n",
    "    print(f\"input lang: {input_lang.name}\")\n",
    "    print(f\"words in input lang: {input_lang.n_words}\")\n",
    "    print()\n",
    "    print(f\"output lang: {output_lang.name}\")\n",
    "    print(f\"words in output lang: {output_lang.n_words}\")\n",
    "    print()\n",
    "    \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embed_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Params.\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Embedding.\n",
    "        self.embedding = nn.Embedding(input_size, embed_size)\n",
    "        \n",
    "        # GRU.\n",
    "        self.gru = nn.GRU(embed_size, hidden_size)\n",
    "    \n",
    "    def forward(self, input_, hidden):\n",
    "        embed = self.embedding(input_)\n",
    "        \n",
    "        embed = embed.view(1, 1, -1)\n",
    "        output, hidden = self.gru(embed, hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)  # device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, embed_size, dropout_p=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Embedding.\n",
    "        self.embedding = nn.Embedding(output_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Attn.\n",
    "        self.attn = Attention(hidden_size)  # receives the size of a context vector\n",
    "        \n",
    "        # GRU.\n",
    "        self.gru = nn.GRU(embed_size + hidden_size, hidden_size)\n",
    "        \n",
    "        # Linear.\n",
    "        self.out = nn.Linear(hidden_size + hidden_size, output_size)\n",
    "#         self.out = nn.Linear(embed_size + hidden_size + hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, last_output, last_hidden, encoder_outputs):\n",
    "        embed = self.embedding(last_output)\n",
    "        embed = self.dropout(embed)\n",
    "        \n",
    "        # Calculates attention weights.\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        # Calcualtes the context vector whose size is [1, hidden_size]\n",
    "        context = attn_weights.mm(encoder_outputs)\n",
    "\n",
    "        # Combines embedded word and attended context.\n",
    "        embed = embed.view(1, 1, -1)\n",
    "        context = context.view(1, 1, -1)\n",
    "        rnn_input = torch.cat((embed, context), 2)\n",
    "        \n",
    "        # s_i = f(c_i, y_{i - 1}, s_{i - 1})\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)  \n",
    "        \n",
    "        # \\hat y_{i} = g(c_i, y_{i - 1}, s_i)\n",
    "        output = self.out(torch.cat((output, context), 2)).view(1, -1)\n",
    "#         output = self.out(torch.cat((output, rnn_input), 2)).view(1, -1)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# # closer to a lesser-used \"location-based\" strategy from Luong et al\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, output_size, hidden_size, embed_size, dropout_p=0.1, max_length=10):\n",
    "#         super(Decoder, self).__init__()\n",
    "        \n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.embed_size = embed_size\n",
    "#         self.output_size = output_size\n",
    "#         self.dropout_p = dropout_p\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#         self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "#         self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "#         self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "#         self.dropout = nn.Dropout(self.dropout_p)\n",
    "#         self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "#         self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "#     def forward(self, input, hidden, encoder_outputs):\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "#         embedded = self.dropout(embedded)\n",
    "\n",
    "#         attn_weights = F.softmax(\n",
    "#             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "#         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "#                                  encoder_outputs.unsqueeze(0))\n",
    "\n",
    "#         output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "#         output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "#         output = F.relu(output)\n",
    "#         output, hidden = self.gru(output, hidden)\n",
    "\n",
    "#         output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "#         return output, hidden, attn_weights\n",
    "\n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, hidden_size):\n",
    "#         super(Attention, self).__init__()\n",
    "                \n",
    "#         # Concat.\n",
    "#         self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "#         self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "        \n",
    "#     def forward(self, hidden, encoder_outputs):        \n",
    "#         # Params.\n",
    "#         seq_len = len(encoder_outputs)\n",
    "        \n",
    "#         attn_energies = torch.zeros(seq_len, device=device)  # device\n",
    "        \n",
    "#         # Calculates energies.\n",
    "#         for i in range(seq_len):\n",
    "#             attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
    "            \n",
    "#         # Normalizes energies.\n",
    "#         attn_energies = F.softmax(attn_energies)\n",
    "        \n",
    "#         # Squeezes.\n",
    "#         attn_energies = attn_energies.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "#         return attn_energies\n",
    "    \n",
    "#     def score(self, hidden, encoder_output):\n",
    "#         energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "# #         print(self.v.size(), energy.size())\n",
    "#         energy = self.v.dot(energy)\n",
    "        \n",
    "#         return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "#         self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "        self.v = nn.Parameter(torch.empty(hidden_size, dtype=torch.float, device=device))\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        hidden = hidden.expand(encoder_output.size(0), -1)\n",
    "        \n",
    "#         max_len * (hidden_size * 2) -> max_len * hidden_size\n",
    "        energy = self.attn(torch.cat((hidden, encoder_output), 1)).tanh()\n",
    "        \n",
    "        # Dot operation.\n",
    "        return torch.sum(self.v * energy, dim=1).view(1, -1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "\n",
    "#         # Transpose max_length and batch_size dimensions\n",
    "#         attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_2_indexes(sentence, lang):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_2_tensor(sentence, lang):\n",
    "    indexes = sentence_2_indexes(sentence, lang)\n",
    "    indexes.append(EOS_token)\n",
    "    \n",
    "    return torch.tensor(indexes, dtype=torch.long, \n",
    "                        device=device).unsqueeze(-1)  # dtype, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_example(pairs, input_lang, output_lang):\n",
    "    pair = random.choice(pairs)\n",
    "    \n",
    "    input_sentence, target_sentence = pair\n",
    "    \n",
    "    input_tensor = sentence_2_tensor(input_sentence, input_lang)\n",
    "    target_tensor = sentence_2_tensor(target_sentence, output_lang)\n",
    "    \n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_format(total_secs):\n",
    "    minutes = int(total_secs / 60)\n",
    "    seconds = total_secs - minutes * 60\n",
    "    \n",
    "    return f\"{minutes:>2.0f}m {seconds:>2.0f}s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_track(start, train_percent):\n",
    "    now = time.time()\n",
    "    \n",
    "    spent_secs = now - start\n",
    "    entire_secs = spent_secs / train_percent\n",
    "    remaining_secs = entire_secs - spent_secs\n",
    "    \n",
    "    return f\"{time_format(spent_secs)} (-{time_format(remaining_secs)})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def plot_losses(losses_plot):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(base=0.2))\n",
    "    \n",
    "    plt.plot(losses_plot)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_(input_, target, encoder, decoder, loss_function, encoder_optimizer, decoder_optimizer, teacher_forcing_ratio, max_len):\n",
    "    # Sees if the decoder is trained with teacher forcing.\n",
    "    teacher_forcing = True if random.random() > teacher_forcing_ratio else False\n",
    "    \n",
    "    # Params.\n",
    "    input_len = len(input_)\n",
    "    target_len = len(target)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # Encoder.\n",
    "    encoder_outputs = torch.zeros(max_len, encoder.hidden_size, device=device)  # device\n",
    "    \n",
    "    # Inits hidden.\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    \n",
    "    # Zeros grad.\n",
    "    encoder_optimizer.zero_grad()  # optim.zerograd(), not model.zero_grad()\n",
    "    \n",
    "    for i in range(input_len):\n",
    "        encoder_output, encoder_hidden = encoder(input_[i], encoder_hidden)\n",
    "        \n",
    "        encoder_outputs[i] = encoder_output[0][0]  # [0][0]\n",
    "        \n",
    "    # Decoder.\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    \n",
    "    # Inits hidden.\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    # Zeros grad.\n",
    "    decoder_optimizer.zero_grad()  # optim.zerograd(), not model.zero_grad()\n",
    "    \n",
    "    if teacher_forcing:\n",
    "        for i in range(target_len):\n",
    "            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            # Calculates the loss.\n",
    "            loss += loss_function(decoder_output, target[i])  # +=\n",
    "            \n",
    "            decoder_input = target[i]\n",
    "    else:\n",
    "        for i in range(target_len):\n",
    "            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            # Calculates the loss.\n",
    "            loss += loss_function(decoder_output, target[i])  # +=\n",
    "            \n",
    "            top_value, top_index = decoder_output.topk(1)\n",
    "            \n",
    "            decoder_input = top_index  # detach?\n",
    "            \n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "                \n",
    "    # Backward.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updates params.\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(encoder, decoder, pairs, input_lang, output_lang, n_iters=75_000, learning_rate=0.01, teacher_forcing_ratio=0.5, \n",
    "          display_every=5_000, plot_every=500, max_len=10):\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    \n",
    "    # Params for tracking the training process.\n",
    "    loss_display = 0\n",
    "    loss_plot = 0\n",
    "    losses_plot = []\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # Loss function.\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    # Optimizers.\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training.\n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        # Gets an example.\n",
    "        input_, target = random_example(pairs, input_lang, output_lang)\n",
    "\n",
    "        # Trains with the example.\n",
    "        loss = train_(input_, target, encoder, decoder, loss_function, encoder_optimizer, decoder_optimizer, teacher_forcing_ratio, \n",
    "                      max_len)\n",
    "        \n",
    "        # Tracks the training process.\n",
    "        loss_display += loss\n",
    "        if iteration % display_every == 0:\n",
    "            avg_loss_display = loss_display / display_every\n",
    "            train_percent = iteration / n_iters\n",
    "            print(f\"iteration: {iteration:>6} {train_percent:>2.0%} {time_track(start, train_percent)} loss: {avg_loss_display:.6f}\")\n",
    "            loss_display = 0\n",
    "            \n",
    "        loss_plot += loss\n",
    "        if iteration % plot_every == 0:\n",
    "            avg_loss_plot = loss_plot / plot_every\n",
    "            losses_plot.append(avg_loss_plot)\n",
    "            loss_plot = 0\n",
    "            \n",
    "    plot_losses(losses_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def plot_attention(attentions, input_sentence, outputs):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    color_ax = ax.matshow(attentions)\n",
    "    fig.colorbar(color_ax)\n",
    "    \n",
    "    ax.set_xticklabels([''] + input_sentence.split(' '), rotation=90)\n",
    "    ax.set_yticklabels([''] + outputs)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_(input_, encoder, decoder, target_lang, max_len):\n",
    "    with torch.no_grad():  # grad not needed\n",
    "        # Params.\n",
    "        input_len = len(input_)\n",
    "        \n",
    "        outputs = []\n",
    "        attentions = torch.zeros(max_len, max_len)  # not needed to be in cuda\n",
    "\n",
    "        # Encoder.\n",
    "        encoder_outputs = torch.zeros(max_len, encoder.hidden_size, device=device)\n",
    "\n",
    "        # Init hidden.\n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "        for i in range(input_len):\n",
    "            encoder_output, encoder_hidden = encoder(input_[i], encoder_hidden)\n",
    "\n",
    "            encoder_outputs[i] = encoder_output[0][0]\n",
    "\n",
    "        # Decoder.\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    \n",
    "        # Init hidden.\n",
    "        decoder_hidden = encoder_hidden\n",
    "    \n",
    "        for i in range(max_len):\n",
    "            decoder_output, decoder_hidden, attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            top_value, top_index = decoder_output.topk(1)\n",
    "            decoder_input = top_index\n",
    "            \n",
    "            attentions[i] = attention\n",
    "            if top_index.item() == EOS_token:\n",
    "                outputs.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                outputs.append(target_lang.index2word[top_index.item()])\n",
    "                \n",
    "        return outputs, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, pairs, input_lang, target_lang, n_evals=5, max_len=10):\n",
    "    for evaluation in range(1, n_evals + 1):\n",
    "        # Gets an example.\n",
    "        input_, target = random_example(pairs, input_lang, target_lang)\n",
    "        \n",
    "        # Evaluates with the example.\n",
    "        outputs, attentions = evaluate_(input_, encoder, decoder, target_lang, max_len)\n",
    "        input_sentence = \" \".join([input_lang.index2word[idx.data.item()] for idx in input_])\n",
    "        target_sentence = \" \".join([target_lang.index2word[idx.data.item()] for idx in target])\n",
    "        \n",
    "        # Displays the results.\n",
    "        print(\"input: \", input_sentence)\n",
    "        print(\"target:\", target_sentence)\n",
    "        print(\"output:\", \" \".join(outputs))\n",
    "        \n",
    "        # Plots attention.\n",
    "        plot_attention(attentions, input_sentence, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing data...\n",
      "input lang: eng\n",
      "words in input lang: 2803\n",
      "\n",
      "output lang: fra\n",
      "words in output lang: 4345\n",
      "\n",
      "Training...\n",
      "iteration:   5000 7%  2m 54s (-40m 34s) loss: 3.142394\n",
      "iteration:  10000 13%  5m 57s (-38m 41s) loss: 2.416975\n",
      "iteration:  15000 20%  9m  4s (-36m 16s) loss: 2.032732\n",
      "iteration:  20000 27% 12m 12s (-33m 34s) loss: 1.726594\n",
      "iteration:  25000 33% 15m 18s (-30m 35s) loss: 1.534338\n",
      "iteration:  30000 40% 18m 21s (-27m 32s) loss: 1.358378\n",
      "iteration:  35000 47% 21m 28s (-24m 32s) loss: 1.203194\n",
      "iteration:  40000 53% 24m 38s (-21m 33s) loss: 1.103568\n",
      "iteration:  45000 60% 27m 41s (-18m 27s) loss: 0.975269\n",
      "iteration:  50000 67% 30m 46s (-15m 23s) loss: 0.934426\n",
      "iteration:  55000 73% 33m 49s (-12m 18s) loss: 0.855136\n",
      "iteration:  60000 80% 36m 55s (- 9m 14s) loss: 0.765815\n",
      "iteration:  65000 87% 39m 57s (- 6m  9s) loss: 0.732690\n",
      "iteration:  70000 93% 43m  3s (- 3m  4s) loss: 0.693448\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Prepares for data.\n",
    "    SOS_token = 0\n",
    "    EOS_token = 1\n",
    "        \n",
    "    input_lang, output_lang, pairs = prepare_data(lang1=\"eng\", lang2=\"fra\")\n",
    "    \n",
    "    # Model.\n",
    "    embed_size = 300\n",
    "    hidden_size = 256\n",
    "    \n",
    "    encoder = Encoder(input_lang.n_words, hidden_size, embed_size).to(device)  # device\n",
    "    decoder = Decoder(output_lang.n_words, hidden_size, embed_size).to(device)  # device\n",
    "    \n",
    "    # Training.\n",
    "    train(encoder, decoder, pairs, input_lang, output_lang)\n",
    "    \n",
    "    # Evaluating.\n",
    "    evaluate(encoder, decoder, pairs, input_lang, output_lang)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
