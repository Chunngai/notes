{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import TranslationDataset\n",
    "\n",
    "class GEC_DATASET(TranslationDataset):\n",
    "    @classmethod\n",
    "    def splits(cls, \n",
    "               exts, \n",
    "               fields, \n",
    "               root, \n",
    "               train=\"train\", \n",
    "               validation=\"dev\", \n",
    "               test=\"test\", \n",
    "               **kwargs):\n",
    "        \n",
    "        return super(GEC_DATASET, cls).splits(exts=exts, \n",
    "                                      fields=fields,\n",
    "                                      path=root, \n",
    "                                      root=root,\n",
    "                                      train=train, \n",
    "                                      validation=validation, \n",
    "                                      test=test,\n",
    "                                      **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "def prepare_data(root):\n",
    "    src_field = Field(init_token=\"<sos>\", eos_token=\"<eos>\", tokenize=lambda sentence: sentence.split(' '))\n",
    "    trg_field = Field(init_token=\"<sos>\", eos_token=\"<eos>\", tokenize=lambda sentence: sentence.split(' '))\n",
    "    \n",
    "    #? Should be lower?\n",
    "    \n",
    "    train_set, valid_set, test_set = GEC_DATASET.splits(exts=('.src', '.trg'), \n",
    "                                                        fields=(src_field, trg_field),\n",
    "                                                        root=root,\n",
    "                                                        filter_pred=lambda sentence: \n",
    "                                                        len(vars(sentence)['src']) < MAX_LEN \n",
    "                                                        and len(vars(sentence)['trg']) < MAX_LEN)\n",
    "\n",
    "    src_field.build_vocab(train_set, min_freq=MIN_FREQ)\n",
    "    trg_field.build_vocab(train_set, min_freq=MIN_FREQ)\n",
    "\n",
    "    train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "        datasets=(train_set, valid_set, test_set), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        device=DEVICE)\n",
    "    \n",
    "    return src_field, trg_field, train_iter, valid_iter, test_iter, train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size):  \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(src_vocab_size, ENC_EMB_DIM)\n",
    "        self.dropout = nn.Dropout(ENC_DROPOUT)  #+ dropout\n",
    "        \n",
    "        self.gru = nn.GRU(ENC_EMB_DIM, ENC_HID_DIM, bidirectional=True)\n",
    "        \n",
    "        self.fc = nn.Linear(ENC_HID_DIM * 2, DEC_HID_DIM)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # (in)  inputs: [src_len, batch_size]\n",
    "        # (out) outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        # (out) hidden: [batch_size, dec_hid_dim]\n",
    "        \n",
    "        # (in)  inputs\n",
    "        # (out) embedded: [src_len, batch_size, enc_emb_dim]\n",
    "        embedded = self.dropout(\n",
    "            self.embedding(inputs))\n",
    "        \n",
    "        # (in)  embedded\n",
    "        # (out) outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        # (out) hiddens: [2, batch_size, enc_hid_dim]\n",
    "        outputs, hiddens = self.gru(embedded)\n",
    "        \n",
    "        # (in)  hiddens\n",
    "        # (out) hidden: [batch_size, dec_hid_dim]\n",
    "        hidden = torch.tanh(\n",
    "            self.fc(\n",
    "                torch.cat((hiddens[0], \n",
    "                           hiddens[1]), dim=1)))\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(DEC_HID_DIM + ENC_HID_DIM * 2, ATTN_V_DIM)\n",
    "        self.v = nn.Parameter(torch.rand(1, ATTN_V_DIM))\n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # (in)  decoder_hidden: [batch_size, dec_hid_dim]\n",
    "        # (in)  encoder_outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        # (out) attn: [batch_size, src_len]\n",
    "        \n",
    "        # (in)  decoder_hidden\n",
    "        # (in)  encoder_outputs\n",
    "        # (out) energy: [batch_size, src_len, attn_v_dim]\n",
    "        energy = torch.tanh(\n",
    "            self.fc(\n",
    "                torch.cat((\n",
    "                    decoder_hidden.unsqueeze(1).repeat(1, encoder_outputs.size()[0], 1), \n",
    "                    encoder_outputs.permute(1, 0, 2)), dim=2)))\n",
    "        \n",
    "        # (in)  v: [1, attn_v_dim]\n",
    "        # (in)  energy\n",
    "        # (out) attn: [batch_size, src_len]\n",
    "        attn = F.softmax(self.v.unsqueeze(0).repeat(energy.size()[0], 1, 1).bmm(energy.permute(0, 2, 1)), dim=2).squeeze(1)  #m\n",
    "        \n",
    "        return attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, trg_vocab_size):  \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(trg_vocab_size, DEC_EMB_DIM)\n",
    "        self.dropout = nn.Dropout(DEC_DROPOUT)  #+ dropout\n",
    "        \n",
    "        self.attn = Attn()\n",
    "        \n",
    "        self.gru = nn.GRU(DEC_EMB_DIM + ENC_HID_DIM * 2, DEC_HID_DIM)\n",
    "        \n",
    "        self.fc = nn.Linear(DEC_EMB_DIM + ENC_HID_DIM * 2 + DEC_HID_DIM, trg_vocab_size)\n",
    "        \n",
    "    def forward(self, last_output, decoder_hidden, encoder_outputs):\n",
    "        # (in)  last_output: [batch_size]\n",
    "        # (in)  decoder_hidden: [batch_size, dec_hid_dim]\n",
    "        # (in)  encoder_outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        #!(out) decoder_outputs: [batch_size, trg_vocab_size]\n",
    "        # (out) decoder_hidden: [batch_size, dec_hid_dim]\n",
    "        \n",
    "        # (in)  last_output\n",
    "        # (out) embedded: [batch_size, dec_emb_dim]\n",
    "        embedded = self.dropout(\n",
    "            self.embedding(last_output))\n",
    "        \n",
    "        # (in)  decoder_hidden\n",
    "        # (in)  encoder_outputs\n",
    "        # (out) attn: [batch_size, src_len]\n",
    "        attn = self.attn(decoder_hidden, encoder_outputs)\n",
    "        # (in)  attn\n",
    "        # (in)  encoder_outputs\n",
    "        # (out) context: [batch, enc_hid_dim * 2]\n",
    "        context = attn.unsqueeze(1).bmm(encoder_outputs.permute(1, 0, 2)).squeeze(1)\n",
    "\n",
    "        # (in)  embedded\n",
    "        # (in)  context\n",
    "        # (in)  decoder_hidden\n",
    "        # (out) outputs: [1, batch_size, dec_hid_dim]\n",
    "        # (out) decoder_hidden: [1, batch_size, dec_hid_dim]\n",
    "        outputs, decoder_hidden = self.gru(\n",
    "            torch.cat((embedded.unsqueeze(0), \n",
    "                       context.unsqueeze(0)), dim=2), \n",
    "            decoder_hidden.unsqueeze(0))\n",
    "        \n",
    "        # (in)  embedded\n",
    "        # (in)  context\n",
    "        # (in)  decoder_hidden\n",
    "        # (out) decoder_outputs: [batch_size, trg_vocab_size]\n",
    "        decoder_outputs = self.fc(\n",
    "            torch.cat((embedded, \n",
    "                       context, \n",
    "                       decoder_hidden.squeeze(0)), dim=1))\n",
    "        \n",
    "        return decoder_outputs, decoder_hidden.squeeze(0), attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(src_vocab_size)\n",
    "        self.decoder = Decoder(trg_vocab_size)\n",
    "        \n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        \n",
    "    def forward(self, inputs, trgs, teacher_forcing_ratio=0.5):\n",
    "        # (in)  inputs: [src_len, batch_size]\n",
    "        # (in)  trgs: [trg_len, batch_size]\n",
    "        # (out) outputs: [trg_len, batch_size, trg_vocab_size]\n",
    "        \n",
    "        # seq len of inputs and trgs may not always be the same\n",
    "                \n",
    "        # Encode.\n",
    "        # (in)  inputs\n",
    "        # (out) encoder_outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        # (out) decoder_hidden: [batch_size, dec_hid_dim]\n",
    "        encoder_outputs, decoder_hidden = self.encoder(inputs)\n",
    "        \n",
    "        # Decode.\n",
    "        trg_len = trgs.size()[0]\n",
    "        batch_size = trgs.size()[1]\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, self.trg_vocab_size, device=DEVICE)\n",
    "\n",
    "        decoder_outputs = trgs[0]\n",
    "        for t in range(1, trg_len):\n",
    "            # (in)  decoder_output: [batch_size]\n",
    "            # (in)  decoder_hidden\n",
    "            # (in)  encoder_outputs\n",
    "            #!(out) decoder_outputs: [batch_size, trg_vocab_size]\n",
    "            # (out) decoder_hidden: [batch_size, dec_hid_dim]\n",
    "            decoder_outputs, decoder_hidden, _ = self.decoder(decoder_outputs, \n",
    "                                                           decoder_hidden, \n",
    "                                                           encoder_outputs)\n",
    "            \n",
    "            outputs[t] = decoder_outputs\n",
    "            \n",
    "            decoder_outputs = decoder_outputs.argmax(dim=1) if teacher_forcing_ratio <= random.random() else trgs[t]\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):  #+ init weights\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train(train_iter, model, criterion, optimizer):\n",
    "    \n",
    "    train_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        \n",
    "        # Gets data.\n",
    "        srcs = batch.src\n",
    "        trgs = batch.trg\n",
    "        \n",
    "        # Forward.\n",
    "        outputs = model(srcs, trgs)\n",
    "        \n",
    "        # Loss.\n",
    "        loss = criterion(outputs[1:].view(-1, outputs.size()[-1]), \n",
    "                         trgs[1:].view(-1))  #m\n",
    "        \n",
    "        # Backward.\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)  #+\n",
    "        \n",
    "        # Updates params\n",
    "        optimizer.step()\n",
    "        # Zeros grad.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    return train_loss / len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(data_iter, model, criterion):\n",
    "    \n",
    "    eval_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter:\n",
    "\n",
    "            # Gets data.\n",
    "            srcs = batch.src\n",
    "            trgs = batch.trg\n",
    "\n",
    "            # Forward.\n",
    "            outputs = model(srcs, trgs, 0)\n",
    "\n",
    "            # Loss.\n",
    "            loss = criterion(outputs[1:].view(-1, outputs.size()[-1]), \n",
    "                             trgs[1:].view(-1))  #m\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "        \n",
    "        return eval_loss / len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_track(start, end):\n",
    "    \n",
    "    elapsed_time = end - start\n",
    "    \n",
    "    mins = int(elapsed_time / 60)\n",
    "    secs = int(elapsed_time % 60)\n",
    "    \n",
    "    return f\"{mins:>2}mins {secs:>2}secs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import copy\n",
    "\n",
    "def train(train_iter, valid_iter, model, criterion, optimizer):\n",
    "        \n",
    "    min_valid_loss = float(\"inf\")  #+\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        train_loss = _train(train_iter, model, criterion, optimizer)\n",
    "        valid_loss = _evaluate(valid_iter, model, criterion)\n",
    "    \n",
    "        end = time.time()\n",
    "        \n",
    "        print(f\"epoch: {epoch + 1:02}, time: {time_track(start, end)}\")\n",
    "        print(f\"train loss: {train_loss:.3f}, train ppl: {math.exp(train_loss):.3f}\")\n",
    "        print(f\"valid loss: {valid_loss:.3f}, valid ppl: {math.exp(valid_loss):.3f}\")\n",
    "        \n",
    "        if valid_loss < min_valid_loss:  #+\n",
    "            min_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), PT)\n",
    "            \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_iter, model, criterion):\n",
    "    model.load_state_dict(torch.load(PT))\n",
    "    \n",
    "    test_loss = _evaluate(test_iter, model, criterion)\n",
    "\n",
    "    print(f\"test loss: {test_loss:.3f}, test ppl: {math.exp(test_loss):.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference  #+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sentence(sentence, src_field, trg_field, model):\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        tokens = [token for token in sentence.split(' ')]\n",
    "    else:\n",
    "        tokens = sentence\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.tensor(src_indexes, dtype=torch.long, device=DEVICE).unsqueeze(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "        \n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "    attentions = torch.zeros(MAX_LEN, 1, len(src_indexes), device=DEVICE)\n",
    "    \n",
    "    for i in range(MAX_LEN):\n",
    "\n",
    "        trg_tensor = torch.tensor([trg_indexes[-1]], dtype=torch.long, device=DEVICE)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
    "            \n",
    "        pred_token = output.argmax(1).item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "        attentions[i] = attention\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, correction, attention):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def inference(dataset, src_field, trg_field, model):\n",
    "    \n",
    "    for _ in range(10):\n",
    "        example_idx = random.randint(1, len(dataset))\n",
    "        \n",
    "        src = vars(dataset.examples[example_idx])['src']\n",
    "        trg = vars(dataset.examples[example_idx])['trg']\n",
    "\n",
    "        print(f\"src = {' '.join(src)}\")\n",
    "        print(f\"trg = {' '.join(trg)}\")\n",
    "        \n",
    "        correction, attention = correct_sentence(src, src_field, trg_field, model)\n",
    "        \n",
    "        print(f\"out = {' '.join(correction[:-1])}\")\n",
    "        \n",
    "        print('---')\n",
    "        \n",
    "#         display_attention(src, correction, attention)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU #+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model):\n",
    "    \n",
    "    trgs = []\n",
    "    outs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        \n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        \n",
    "        out, _ = correct_sentence(src, src_field, trg_field, model)\n",
    "        \n",
    "        #cut off <eos> token\n",
    "        out = out[:-1]\n",
    "        \n",
    "        outs.append(out)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "    return bleu_score(outs, trgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, time: 29mins  5secs\n",
      "train loss: 2.808, train ppl: 16.572\n",
      "valid loss: 2.216, valid ppl: 9.168\n",
      "epoch: 02, time: 28mins 30secs\n",
      "train loss: 1.411, train ppl: 4.102\n",
      "valid loss: 2.023, valid ppl: 7.558\n",
      "epoch: 03, time: 28mins 56secs\n",
      "train loss: 1.126, train ppl: 3.083\n",
      "valid loss: 2.081, valid ppl: 8.013\n",
      "epoch: 04, time: 29mins 19secs\n",
      "train loss: 0.946, train ppl: 2.574\n",
      "valid loss: 2.094, valid ppl: 8.120\n",
      "epoch: 05, time: 28mins 46secs\n",
      "train loss: 0.808, train ppl: 2.243\n",
      "valid loss: 2.207, valid ppl: 9.084\n",
      "\n",
      "test loss: 2.370, test ppl: 10.695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    DATASET = \"../data/fce+lang8(partial) with bpe (train seq num: 93112)\"\n",
    "    \n",
    "    # Prepares for data.\n",
    "    ROOT = f\"{DATASET}/parallel\"\n",
    "    MAX_LEN = 100\n",
    "    MIN_FREQ = 1\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    src_field, trg_field, train_iter, valid_iter, test_iter, train_set, valid_set, test_set = prepare_data(root=ROOT)\n",
    "    \n",
    "    # Model.\n",
    "    ENC_EMB_DIM = 600\n",
    "    ENC_HID_DIM = 1_000\n",
    "    ENC_DROPOUT = 0.5\n",
    "\n",
    "    DEC_EMB_DIM = 600\n",
    "    DEC_HID_DIM = 1_000\n",
    "    DEC_DROPOUT = 0.5\n",
    "    \n",
    "    ATTN_V_DIM = 1_000  #m\n",
    "    \n",
    "    model = Seq2Seq(len(src_field.vocab), len(trg_field.vocab)).to(DEVICE)\n",
    "    model.apply(init_weights)\n",
    "    \n",
    "    # Criterion.\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=trg_field.vocab.stoi[\"<pad>\"])\n",
    "    \n",
    "    # Optimizer.\n",
    "    LR = 0.0003\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)  #n Decreasing lr makes the change of valid_loss slowlier, thus easier to get the optimal?\n",
    "    \n",
    "    # Trains and validates.\n",
    "    N_EPOCHS = 10\n",
    "    CLIP = 1  #+\n",
    "    PT = f\"{DATASET}.pt\"\n",
    "    \n",
    "    train(train_iter, valid_iter, model, criterion, optimizer)\n",
    "    \n",
    "    # Tests.\n",
    "    test(test_iter, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training inference\n",
      "src = From that time , I decided to buy my own computer and every day I was improving my knowledge . Firstly I become to write letters and articles , next I learned several computers plays .\n",
      "trg = At that time , I decided to buy my own computer and every day I was improving my knowledge . First I came to write letters and articles , next I learned several computer games .\n",
      "out = From that time , I decided to buy my own computer and every day I was improving my knowledge . First I become to write letters and articles , next I learned several computers plays .\n",
      "---\n",
      "src = It is a main place where mountain games will take place in 2010 Vancouver Olympic .\n",
      "trg = It is the main place where mountain games will take place in the 2010 Vancouver Olympics .\n",
      "out = It is a main place where mountain games will take place in 2010 Vancouver .\n",
      "---\n",
      "src = It 's the tou@@ gh@@ est of the all training ! !\n",
      "trg = It 's the tou@@ gh@@ est of all train@@ ings ! !\n",
      "out = It 's the tou@@ gh@@ est of the all of ! !\n",
      "---\n",
      "src = Im@@ medi@@ ately she interru@@ p@@ ted and told to the man that she was a secret ag@@ ent .\n",
      "trg = Im@@ medi@@ ately she interru@@ p@@ ted and told the man that she was a secret ag@@ ent .\n",
      "out = Im@@ medi@@ ately she interru@@ p@@ ted and told to the man that she was a secret ag@@ ent .\n",
      "---\n",
      "src = Today is Festival day near my home .\n",
      "trg = Today is Festival day near my home .\n",
      "out = Today is a day near my home .\n",
      "---\n",
      "src = But it 's too late for my students and me to prepare .\n",
      "trg = But it 's too late for my students and I to prepare .\n",
      "out = But it 's too late for my students and me to prepare .\n",
      "---\n",
      "src = And also I want to ask some questions .\n",
      "trg = And also I want to ask some questions .\n",
      "out = And also I want to ask some questions .\n",
      "---\n",
      "src = Last night was a second date I arranged for my friend and hopefully they can work things out !\n",
      "trg = Last night was the second date ( that ) I arranged for my friend and hopefully they can work things out !\n",
      "out = Last night was a second date I arranged for my friend and hopefully they can work things out !\n",
      "---\n",
      "src = I think it could be very interesting and it is free for students ( what is really good ) .\n",
      "trg = I think it could be very interesting and it is free for students ( which is really good ) .\n",
      "out = I think it could be very interesting and it is free for students ( what is really good ) .\n",
      "---\n",
      "src = Fa@@ thermore , discounts was un@@ available .\n",
      "trg = Furthermore , no discounts were available .\n",
      "out = Finally , discounts were un@@ available .\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"training inference\")\n",
    "inference(train_set, src_field, trg_field, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating inference\n",
      "src = Further more , yesterday I 've heard that every man could fly to the space without any experience , it is great , from my point of view in 20@@ 50 year everybody will fly to the space .\n",
      "trg = Furthermore , yesterday I heard that every man could fly into space without any experience , it is great , from my point of view , that in the year 20@@ 50 everybody will fly into space .\n",
      "out = Furthermore , yesterday I 've heard that every man could fly to the space without any experience , it is great , from my point of view of 20@@ 50 years everybody will fly to the space .\n",
      "---\n",
      "src = Looking forward to hearing from you\n",
      "trg = Looking forward to hearing from you\n",
      "out = Looking forward to hearing from you\n",
      "---\n",
      "src = Yours sincerely\n",
      "trg = Yours sincerely\n",
      "out = Yours sincerely\n",
      "---\n",
      "src = Thank you very much for your letter , I was glad to get it and know a lot of news about you .\n",
      "trg = Thank you very much for your letter . I was glad to get it and get a lot of news about you .\n",
      "out = Thank you very much for your letter , I was glad to get it and know a lot of news about you .\n",
      "---\n",
      "src = I THANK HI@@ M SO M@@ UC@@ H , AND I WAS SO HAPP@@ Y THAT I ASK@@ ED TO THE RI@@ G@@ HT PER@@ S@@ ON AND NO@@ THING W@@ R@@ ON HAPP@@ EN TO ME .\n",
      "trg = I THAN@@ K@@ ED HI@@ M VERY M@@ UC@@ H , AND I WAS SO HAPP@@ Y THAT I ASK@@ ED THE RI@@ G@@ HT PER@@ S@@ ON AND NO@@ THING BA@@ D HAPP@@ EN@@ ED TO ME .\n",
      "out = I WOULD HI@@ M SO M@@ UC@@ H , AND I WAS SO HAPP@@ Y THAT I ASK@@ ED TO THE RI@@ G@@ HT AL S@@ ON AND NO@@ LY W@@ R@@ ON HAPP@@ EN TO ME .\n",
      "---\n",
      "src = The whole class is also very excited about this fashion show and would really appreciate your giving them a change to go .\n",
      "trg = The whole class is also very excited about this fashion show and would really appreciate your giving them a chance to go .\n",
      "out = The whole class is also very excited about this fashion show and would really appreciate your giving them change change .\n",
      "---\n",
      "src = Is there any toilet and shower in the log cabins or somewhere nearby ?\n",
      "trg = Is there a toilet and shower in the log cabins or somewhere nearby ?\n",
      "out = Is there any toilet and shower in the log cabins or somewhere nearby ?\n",
      "---\n",
      "src = It may in many cases , not be true but we can susp@@ ect that most of them wanted to become a cele@@ br@@ ity and they had to know there is no private life un@@ separ@@ ately cont@@ ected to it .\n",
      "trg = It may , in many cases , not be true but we can susp@@ ect that most of them wanted to become a cele@@ br@@ ity and they had to know there is no private life un@@ separ@@ ately connected to it .\n",
      "out = It may in many cases , not be true but we can susp@@ ect that most of them wanted to become a tr@@ br@@ ity and they had to know there is no private life un@@ consc@@ ately cont@@ ected to it .\n",
      "---\n",
      "src = But his nose moved quickly when police showed a picture of bank rob@@ bers .\n",
      "trg = But his nose moved quickly when the police showed him a picture of the bank rob@@ bers .\n",
      "out = But his nose moved quickly when police showed a picture of the rob@@ bers bers .\n",
      "---\n",
      "src = I am very disappointed because the things that were writ@@ en on the advertisment were not really true .\n",
      "trg = I am very disappointed because the things that were written in the advertisement were not really true .\n",
      "out = I am very disappointed because the things that were written in the advertisement were not really true .\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"validating inference\")\n",
    "inference(valid_set, src_field, trg_field, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing inference\n",
      "src = Let us try to appreciate thier importance again .\n",
      "trg = Let us try to appreciate their importance again .\n",
      "out = Let us try to appreciate thier importance again .\n",
      "---\n",
      "src = But really you must see it , the position of the ca@@ the@@ dra@@ l give you a som@@ en@@ thing special a big tran@@ qui@@ lity .\n",
      "trg = But really you must see it . The position of the ca@@ the@@ dra@@ l gives you a special something a great tran@@ qu@@ il@@ lity .\n",
      "out = But really you must see it , the position of the ca@@ the@@ ance l give you a som@@ en@@ thing special a big tran@@ qui@@ .\n",
      "---\n",
      "src = P@@ op@@ u@@ lon@@ i@@ ou is about 15 K@@ m . from Pi@@ om@@ b@@ ino and it can be reached by well - organised means of transport .\n",
      "trg = P@@ op@@ u@@ lon@@ i@@ ou is about 15 K@@ m . from Pi@@ om@@ b@@ ino and it can be reached by a well - organised means of transport .\n",
      "out = P@@ op@@ u@@ lon@@ i@@ ou is about 15 K@@ m . from Pi@@ om@@ b@@ ino and it can be reached by well - organised means of transport .\n",
      "---\n",
      "src = Also there is a very good shopping center next to the museum .\n",
      "trg = Also there is a very good shopping centre next to the museum .\n",
      "out = Also there is a very good shopping center next to the museum .\n",
      "---\n",
      "src = I 'm afraid for Richard .\n",
      "trg = I 'm sorry to hear about Richard .\n",
      "out = I 'm afraid for Richard .\n",
      "---\n",
      "src = I hope I have given you enough information and if you still have some doubts , please do not h@@ es@@ tit@@ ate to contact us .\n",
      "trg = I hope I have given you enough information and if you still have some questions , please do not hesitate to contact us .\n",
      "out = I hope I have given you enough information and if you still have some , , please do not h@@ es@@ consc@@ ate to contact us .\n",
      "---\n",
      "src = F@@ AC@@ I@@ LI@@ TIES :\n",
      "trg = F@@ AC@@ I@@ LI@@ TIES :\n",
      "out = F@@ AC@@ I@@ LI@@ VE :\n",
      "---\n",
      "src = I have made arrangement for the afternoon on the last day for you .\n",
      "trg = I have made arrang@@ ements for the afternoon on the last day for you .\n",
      "out = I have made arrangement for the afternoon on the last day for you .\n",
      "---\n",
      "src = Can you deny how quickly we get somewhere by car or by bus .\n",
      "trg = Can you deny how quickly we get somewhere by car or by bus ?\n",
      "out = Can you deny how quickly we get somewhere by car by by bus by bus by bus by bus .\n",
      "---\n",
      "src = I am sure that should be en@@ ought for one day trip .\n",
      "trg = I am sure that should be enough for a one - day trip .\n",
      "out = I am sure that should be en@@ ought for one day trip .\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"testing inference\")\n",
    "inference(test_set, src_field, trg_field, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 69.64\n"
     ]
    }
   ],
   "source": [
    "bleu = calculate_bleu(test_set, src_field, trg_field, model)\n",
    "print(f'BLEU score = {bleu*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
