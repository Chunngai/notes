{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import TranslationDataset\n",
    "\n",
    "class FCE(TranslationDataset):\n",
    "    @classmethod\n",
    "    def splits(cls, \n",
    "               exts, \n",
    "               fields, \n",
    "               root=\"fce/m2\", \n",
    "               train=\"fce.train.gold.bea19\", \n",
    "               validation=\"fce.dev.gold.bea19\", \n",
    "               test=\"fce.test.gold.bea19\", \n",
    "               **kwargs):\n",
    "        \n",
    "        return super(FCE, cls).splits(exts=exts, \n",
    "                                      fields=fields,\n",
    "                                      path=root, \n",
    "                                      root=root,\n",
    "                                      train=train, \n",
    "                                      validation=validation, \n",
    "                                      test=test,\n",
    "                                      **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "def prepare_data():\n",
    "    src_field = Field(init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, \n",
    "                     tokenize=\"spacy\", tokenizer_language=\"en\")\n",
    "    trg_field = Field(init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, \n",
    "                     tokenize=\"spacy\", tokenizer_language=\"en\")\n",
    "    #? Should be lower?\n",
    "    \n",
    "    train_set, valid_set, test_set = FCE.splits(exts=('.src', '.trg'), \n",
    "                                               fields=(src_field, trg_field),\n",
    "                                               filter_pred=lambda sentence: \n",
    "                                                len(vars(sentence)['src']) < MAX_LEN \n",
    "                                                and len(vars(sentence)['trg']) < MAX_LEN)\n",
    "\n",
    "    src_field.build_vocab(train_set, min_freq=MIN_FREQ)\n",
    "    trg_field.build_vocab(train_set, min_freq=MIN_FREQ)\n",
    "\n",
    "    train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "        datasets=(train_set, valid_set, test_set), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        device=DEVICE)\n",
    "    \n",
    "    return src_field, trg_field, train_iter, valid_iter, test_iter, train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size):  \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(src_vocab_size, ENC_EMB_DIM)\n",
    "        self.dropout = nn.Dropout(ENC_DROPOUT)  #+ dropout\n",
    "        \n",
    "        self.gru = nn.GRU(ENC_EMB_DIM, ENC_HID_DIM, bidirectional=True)\n",
    "        \n",
    "        self.fc = nn.Linear(ENC_HID_DIM * 2, DEC_HID_DIM)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # (in)  inputs: [src_len, batch_size]\n",
    "        # (out) outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        # (out) hidden: [batch_size, dec_hid_dim]\n",
    "        \n",
    "        # (in)  inputs\n",
    "        # (out) embedded: [src_len, batch_size, enc_emb_dim]\n",
    "        embedded = self.dropout(\n",
    "            self.embedding(inputs))\n",
    "        \n",
    "        # (in)  embedded\n",
    "        # (out) outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        # (out) hiddens: [2, batch_size, enc_hid_dim]\n",
    "        outputs, hiddens = self.gru(embedded)\n",
    "        \n",
    "        # (in)  hiddens\n",
    "        # (out) hidden: [batch_size, dec_hid_dim]\n",
    "        hidden = torch.tanh(\n",
    "            self.fc(\n",
    "                torch.cat((hiddens[0], \n",
    "                           hiddens[1]), dim=1)))\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(DEC_HID_DIM + ENC_HID_DIM * 2, ATTN_V_DIM)\n",
    "        self.v = nn.Parameter(torch.rand(1, ATTN_V_DIM))\n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # (in)  decoder_hidden: [batch_size, dec_hid_dim]\n",
    "        # (in)  encoder_outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        # (out) attn: [batch_size, src_len]\n",
    "        \n",
    "        # (in)  decoder_hidden\n",
    "        # (in)  encoder_outputs\n",
    "        # (out) energy: [batch_size, src_len, attn_v_dim]\n",
    "        energy = torch.tanh(\n",
    "            self.fc(\n",
    "                torch.cat((\n",
    "                    decoder_hidden.unsqueeze(1).repeat(1, encoder_outputs.size()[0], 1), \n",
    "                    encoder_outputs.permute(1, 0, 2)), dim=2)))\n",
    "        \n",
    "        # (in)  v: [1, attn_v_dim]\n",
    "        # (in)  energy\n",
    "        # (out) attn: [batch_size, src_len]\n",
    "        attn = F.softmax(self.v.unsqueeze(0).repeat(energy.size()[0], 1, 1).bmm(energy.permute(0, 2, 1)), dim=2).squeeze(1)  #m\n",
    "        \n",
    "        return attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, trg_vocab_size):  \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(trg_vocab_size, DEC_EMB_DIM)\n",
    "        self.dropout = nn.Dropout(DEC_DROPOUT)  #+ dropout\n",
    "        \n",
    "        self.attn = Attn()\n",
    "        \n",
    "        self.gru = nn.GRU(DEC_EMB_DIM + ENC_HID_DIM * 2, DEC_HID_DIM)\n",
    "        \n",
    "        self.fc = nn.Linear(DEC_EMB_DIM + ENC_HID_DIM * 2 + DEC_HID_DIM, trg_vocab_size)\n",
    "        \n",
    "    def forward(self, last_output, decoder_hidden, encoder_outputs):\n",
    "        # (in)  last_output: [batch_size]\n",
    "        # (in)  decoder_hidden: [batch_size, dec_hid_dim]\n",
    "        # (in)  encoder_outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        #!(out) decoder_outputs: [batch_size, trg_vocab_size]\n",
    "        # (out) decoder_hidden: [batch_size, dec_hid_dim]\n",
    "        \n",
    "        # (in)  last_output\n",
    "        # (out) embedded: [batch_size, dec_emb_dim]\n",
    "        embedded = self.dropout(\n",
    "            self.embedding(last_output))\n",
    "        \n",
    "        # (in)  decoder_hidden\n",
    "        # (in)  encoder_outputs\n",
    "        # (out) attn: [batch_size, src_len]\n",
    "        attn = self.attn(decoder_hidden, encoder_outputs)\n",
    "        # (in)  attn\n",
    "        # (in)  encoder_outputs\n",
    "        # (out) context: [batch, enc_hid_dim * 2]\n",
    "        context = attn.unsqueeze(1).bmm(encoder_outputs.permute(1, 0, 2)).squeeze(1)\n",
    "\n",
    "        # (in)  embedded\n",
    "        # (in)  context\n",
    "        # (in)  decoder_hidden\n",
    "        # (out) outputs: [1, batch_size, dec_hid_dim]\n",
    "        # (out) decoder_hidden: [1, batch_size, dec_hid_dim]\n",
    "        outputs, decoder_hidden = self.gru(\n",
    "            torch.cat((embedded.unsqueeze(0), \n",
    "                       context.unsqueeze(0)), dim=2), \n",
    "            decoder_hidden.unsqueeze(0))\n",
    "        \n",
    "        # (in)  embedded\n",
    "        # (in)  context\n",
    "        # (in)  decoder_hidden\n",
    "        # (out) decoder_outputs: [batch_size, trg_vocab_size]\n",
    "        decoder_outputs = self.fc(\n",
    "            torch.cat((embedded, \n",
    "                       context, \n",
    "                       decoder_hidden.squeeze(0)), dim=1))\n",
    "        \n",
    "        return decoder_outputs, decoder_hidden.squeeze(0), attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(src_vocab_size)\n",
    "        self.decoder = Decoder(trg_vocab_size)\n",
    "        \n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        \n",
    "    def forward(self, inputs, trgs, teacher_forcing_ratio=0.5):\n",
    "        # (in)  inputs: [src_len, batch_size]\n",
    "        # (in)  trgs: [trg_len, batch_size]\n",
    "        # (out) outputs: [trg_len, batch_size, trg_vocab_size]\n",
    "        \n",
    "        # seq len of inputs and trgs may not always be the same\n",
    "                \n",
    "        # Encode.\n",
    "        # (in)  inputs\n",
    "        # (out) encoder_outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        # (out) decoder_hidden: [batch_size, dec_hid_dim]\n",
    "        encoder_outputs, decoder_hidden = self.encoder(inputs)\n",
    "        \n",
    "        # Decode.\n",
    "        trg_len = trgs.size()[0]\n",
    "        batch_size = trgs.size()[1]\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, self.trg_vocab_size, device=DEVICE)\n",
    "\n",
    "        decoder_outputs = trgs[0]\n",
    "        for t in range(1, trg_len):\n",
    "            # (in)  decoder_output: [batch_size]\n",
    "            # (in)  decoder_hidden\n",
    "            # (in)  encoder_outputs\n",
    "            #!(out) decoder_outputs: [batch_size, trg_vocab_size]\n",
    "            # (out) decoder_hidden: [batch_size, dec_hid_dim]\n",
    "            decoder_outputs, decoder_hidden, _ = self.decoder(decoder_outputs, \n",
    "                                                           decoder_hidden, \n",
    "                                                           encoder_outputs)\n",
    "            \n",
    "            outputs[t] = decoder_outputs\n",
    "            \n",
    "            decoder_outputs = decoder_outputs.argmax(dim=1) if teacher_forcing_ratio <= random.random() else trgs[t]\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):  #+ init weights\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train(train_iter, model, criterion, optimizer):\n",
    "    \n",
    "    train_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        \n",
    "        # Gets data.\n",
    "        srcs = batch.src\n",
    "        trgs = batch.trg\n",
    "        \n",
    "        # Forward.\n",
    "        outputs = model(srcs, trgs)\n",
    "        \n",
    "        # Loss.\n",
    "        loss = criterion(outputs[1:].view(-1, outputs.size()[-1]), \n",
    "                         trgs[1:].view(-1))  #m\n",
    "        \n",
    "        # Backward.\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)  #+\n",
    "        \n",
    "        # Updates params\n",
    "        optimizer.step()\n",
    "        # Zeros grad.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    return train_loss / len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(data_iter, model, criterion):\n",
    "    \n",
    "    eval_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter:\n",
    "\n",
    "            # Gets data.\n",
    "            srcs = batch.src\n",
    "            trgs = batch.trg\n",
    "\n",
    "            # Forward.\n",
    "            outputs = model(srcs, trgs, 0)\n",
    "\n",
    "            # Loss.\n",
    "            loss = criterion(outputs[1:].view(-1, outputs.size()[-1]), \n",
    "                             trgs[1:].view(-1))  #m\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "        \n",
    "        return eval_loss / len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_track(start, end):\n",
    "    \n",
    "    elapsed_time = end - start\n",
    "    \n",
    "    mins = int(elapsed_time / 60)\n",
    "    secs = int(elapsed_time % 60)\n",
    "    \n",
    "    return f\"{mins:>2}mins {secs:>2}secs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import copy\n",
    "\n",
    "def train(train_iter, valid_iter, model, criterion, optimizer):\n",
    "        \n",
    "    min_valid_loss = float(\"inf\")  #+\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        train_loss = _train(train_iter, model, criterion, optimizer)\n",
    "        valid_loss = _evaluate(valid_iter, model, criterion)\n",
    "    \n",
    "        end = time.time()\n",
    "        \n",
    "        print(f\"epoch: {epoch + 1:02}, time: {time_track(start, end)}\")\n",
    "        print(f\"train loss: {train_loss:.3f}, train ppl: {math.exp(train_loss):.3f}\")\n",
    "        print(f\"valid loss: {valid_loss:.3f}, valid ppl: {math.exp(valid_loss):.3f}\")\n",
    "        \n",
    "        if valid_loss < min_valid_loss:  #+\n",
    "            min_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'fce.pt')\n",
    "            \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_iter, model, criterion):\n",
    "    model.load_state_dict(torch.load(\"fce.pt\"))\n",
    "    \n",
    "    test_loss = _evaluate(test_iter, model, criterion)\n",
    "\n",
    "    print(f\"test loss: {test_loss:.3f}, test ppl: {math.exp(test_loss):.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference  #+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def correct_sentence(sentence, src_field, trg_field, model):\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        tokenizer = spacy.load('en')\n",
    "        tokens = [token.text.lower() for token in tokenizer(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.tensor(src_indexes, dtype=torch.long, device=DEVICE).unsqueeze(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "        \n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "    attentions = torch.zeros(MAX_LEN, 1, len(src_indexes), device=DEVICE)\n",
    "    \n",
    "    for i in range(MAX_LEN):\n",
    "\n",
    "        trg_tensor = torch.tensor([trg_indexes[-1]], dtype=torch.long, device=DEVICE)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
    "            \n",
    "        pred_token = output.argmax(1).item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "        attentions[i] = attention\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, correction, attention):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def inference(dataset, src_field, trg_field, model):\n",
    "    \n",
    "    for _ in range(10):\n",
    "        example_idx = random.randint(1, len(dataset))\n",
    "        \n",
    "        src = vars(dataset.examples[example_idx])['src']\n",
    "        trg = vars(dataset.examples[example_idx])['trg']\n",
    "\n",
    "        print(f\"src = {' '.join(src)}\")\n",
    "        print(f\"trg = {' '.join(trg)}\")\n",
    "        \n",
    "        correction, attention = correct_sentence(src, src_field, trg_field, model)\n",
    "        \n",
    "        print(f\"out = {' '.join(correction[:-1])}\")\n",
    "        \n",
    "        print('---')\n",
    "        \n",
    "#         display_attention(src, correction, attention)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU #+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model):\n",
    "    \n",
    "    trgs = []\n",
    "    outs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        \n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        \n",
    "        out, _ = correct_sentence(src, src_field, trg_field, model)\n",
    "        \n",
    "        #cut off <eos> token\n",
    "        out = out[:-1]\n",
    "        \n",
    "        outs.append(out)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "    return bleu_score(outs, trgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 01, time:  3mins 55secs\n",
      "train loss: 4.830, train ppl: 125.272\n",
      "valid loss: 3.764, valid ppl: 43.130\n",
      "epoch: 02, time:  3mins 42secs\n",
      "train loss: 2.650, train ppl: 14.150\n",
      "valid loss: 2.396, valid ppl: 10.984\n",
      "epoch: 03, time:  3mins 33secs\n",
      "train loss: 1.630, train ppl: 5.103\n",
      "valid loss: 2.101, valid ppl: 8.176\n",
      "epoch: 04, time:  3mins 37secs\n",
      "train loss: 1.235, train ppl: 3.437\n",
      "valid loss: 2.023, valid ppl: 7.557\n",
      "epoch: 05, time:  3mins 37secs\n",
      "train loss: 1.003, train ppl: 2.727\n",
      "valid loss: 1.924, valid ppl: 6.849\n",
      "epoch: 06, time:  3mins 37secs\n",
      "train loss: 0.836, train ppl: 2.308\n",
      "valid loss: 1.922, valid ppl: 6.832\n",
      "epoch: 07, time:  3mins 33secs\n",
      "train loss: 0.735, train ppl: 2.086\n",
      "valid loss: 1.880, valid ppl: 6.554\n",
      "epoch: 08, time:  3mins 35secs\n",
      "train loss: 0.636, train ppl: 1.889\n",
      "valid loss: 1.946, valid ppl: 7.000\n",
      "epoch: 09, time:  3mins 32secs\n",
      "train loss: 0.560, train ppl: 1.751\n",
      "valid loss: 1.953, valid ppl: 7.049\n",
      "epoch: 10, time:  3mins 34secs\n",
      "train loss: 0.483, train ppl: 1.622\n",
      "valid loss: 2.012, valid ppl: 7.479\n",
      "epoch: 11, time:  3mins 29secs\n",
      "train loss: 0.424, train ppl: 1.528\n",
      "valid loss: 2.069, valid ppl: 7.913\n",
      "epoch: 12, time:  3mins 29secs\n",
      "train loss: 0.372, train ppl: 1.451\n",
      "valid loss: 2.105, valid ppl: 8.204\n",
      "epoch: 13, time:  3mins 30secs\n",
      "train loss: 0.327, train ppl: 1.387\n",
      "valid loss: 2.248, valid ppl: 9.470\n",
      "epoch: 14, time:  3mins 29secs\n",
      "train loss: 0.286, train ppl: 1.331\n",
      "valid loss: 2.218, valid ppl: 9.185\n",
      "epoch: 15, time:  3mins 27secs\n",
      "train loss: 0.244, train ppl: 1.276\n",
      "valid loss: 2.278, valid ppl: 9.754\n",
      "\n",
      "test loss: 2.224, test ppl: 9.244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    \n",
    "    # Prepares for data.\n",
    "    MAX_LEN = 200\n",
    "    MIN_FREQ = 2\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    src_field, trg_field, train_iter, valid_iter, test_iter, train_set, valid_set, test_set = prepare_data()\n",
    "    \n",
    "    # Model.\n",
    "    ENC_EMB_DIM = 256\n",
    "    ENC_HID_DIM = 512\n",
    "    ENC_DROPOUT = 0.5\n",
    "    ENC_DROPOUT = 0\n",
    "\n",
    "    DEC_EMB_DIM = 256\n",
    "    DEC_HID_DIM = 512\n",
    "    DEC_DROPOUT = 0.5\n",
    "    DEC_DROPOUT = 0\n",
    "    \n",
    "    ATTN_V_DIM = DEC_HID_DIM  #m\n",
    "    \n",
    "    model = Seq2Seq(len(src_field.vocab), len(trg_field.vocab)).to(DEVICE)\n",
    "    model.apply(init_weights)\n",
    "    \n",
    "    # Criterion.\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=trg_field.vocab.stoi[\"<pad>\"])\n",
    "    \n",
    "    # Optimizer.\n",
    "    LR = 0.0005\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)  #n Decreasing lr makes the change of valid_loss slowlier, thus easier to get the optimal?\n",
    "    \n",
    "    # Trains and validates.\n",
    "    N_EPOCHS = 15\n",
    "    CLIP = 1  #+\n",
    "    \n",
    "    train(train_iter, valid_iter, model, criterion, optimizer)\n",
    "    \n",
    "    # Tests.\n",
    "    test(test_iter, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training inference\n",
      "src = i would like to write about my experience that i had , when i enjoyed helping at a pop concert in my neighbourhood .\n",
      "trg = i would like to write about the experience that i had when i enjoyed helping at a pop concert in my neighbourhood .\n",
      "out = i would like to write about my experience that i had when i enjoyed helping at a pop concert in my neighbourhood .\n",
      "---\n",
      "src = it was the worse holiday in my life but we continued .\n",
      "trg = it was already the worst holiday in my life but we continued .\n",
      "out = it was the worst holiday in my life but we continued .\n",
      "---\n",
      "src = we need your help for the details .\n",
      "trg = we need your help with the details .\n",
      "out = we need your help for the details .\n",
      "---\n",
      "src = we all agree that we are going to join the school trip to science museum , which is the most interesting trip for us .\n",
      "trg = we all agree that we are going on the school trip to the science museum , which is the most interesting trip for us .\n",
      "out = we all agree that we are going to join the school trip to the science museum , which is the most interesting trip for us .\n",
      "---\n",
      "src = and i 'd like to know about what kind of clothes to take and how much money to take too .\n",
      "trg = and i 'd like to know what kind of clothes to take and how much money to take too .\n",
      "out = and i 'd like to know about what kind of clothes to take and how much money to take too .\n",
      "---\n",
      "src = unfortunately , pat was n't very good at keeping secrets . i had known that but i told my secret to her , which was a big mistake .\n",
      "trg = unfortunately , pat was n't very good at keeping secrets . i had known that but i told her my secret , which was a big mistake .\n",
      "out = unfortunately , pat was n't very good at keeping secrets . i had known that but i told my secret to her , which was a big mistake .\n",
      "---\n",
      "src = kyon chwa\n",
      "trg = kyon chwa\n",
      "out = <unk> <unk> <unk>\n",
      "---\n",
      "src = it is false , i have paid twenty pounds .\n",
      "trg = it was untrue . i paid twenty pounds .\n",
      "out = it is untrue , i have paid pounds pounds pounds pounds pounds pounds pounds pounds pounds pounds pounds pounds pounds pounds pounds pounds pounds pounds pounds pounds .\n",
      "---\n",
      "src = dear mrs brown ,\n",
      "trg = dear mrs brown ,\n",
      "out = dear mrs brown ,\n",
      "---\n",
      "src = that is why i will be pleased if you give my money back .\n",
      "trg = that is why i will be pleased if you give me my money back .\n",
      "out = that is why i will be pleased if you give me my money back .\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"training inference\")\n",
    "inference(train_set, src_field, trg_field, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating inference\n",
      "src = every school has it 's own rules .\n",
      "trg = every school has its own rules .\n",
      "out = every school has it 's own .\n",
      "---\n",
      "src = very often a shop assistant is not helpful so you lose your money and your time .\n",
      "trg = very often a shop assistant is not helpful so you lose your money and your time .\n",
      "out = the often a shop assistant is not helpful so you waste waste your money and your time .\n",
      "---\n",
      "src = emilio almodar , 18 years old , university student , and the career i have choosen is internacional commerce or market .\n",
      "trg = emilio almodar , 18 years old , university student , and the career i have chosen is in international commerce or market .\n",
      "out = <unk> <unk> , , the years old , university , and the career i have chosen <unk> <unk> or market .\n",
      "---\n",
      "src = your faithfully ,\n",
      "trg = yours faithfully ,\n",
      "out = yours faithfully ,\n",
      "---\n",
      "src = famous people x media\n",
      "trg = famous people & the media\n",
      "out = famous people x media\n",
      "---\n",
      "src = i know it 's my fault .\n",
      "trg = i know it 's my fault .\n",
      "out = i know it 's my fault .\n",
      "---\n",
      "src = i want to thank you for this prize , let me know if you need further information .\n",
      "trg = i want to thank you for this prize . let me know if you need further information .\n",
      "out = i want to thank you for this prize , let me know if you need further information .\n",
      "---\n",
      "src = tips to earn money\n",
      "trg = tips for earning money\n",
      "out = probably to earn money money\n",
      "---\n",
      "src = pat saw it !\n",
      "trg = pat saw it !\n",
      "out = pat saw it !\n",
      "---\n",
      "src = they are imported from enother western europe country .\n",
      "trg = they are imported from another western european country .\n",
      "out = they are <unk> from <unk> <unk> several country .\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"validating inference\")\n",
    "inference(valid_set, src_field, trg_field, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing inference\n",
      "src = this is an informal party .\n",
      "trg = this is an informal party .\n",
      "out = this is an <unk> party .\n",
      "---\n",
      "src = yours sincerely ,\n",
      "trg = yours faithfully ,\n",
      "out = yours sincerely ,\n",
      "---\n",
      "src = small things but very importants in our lifes .\n",
      "trg = small things but very important in our lives .\n",
      "out = small things but very <unk> in our lives .\n",
      "---\n",
      "src = i am writing to give you some advice of an international student conference .\n",
      "trg = i am writing to give you some advice about the international student conference .\n",
      "out = i am writing to give you some advice about an international student .\n",
      "---\n",
      "src = that should complete that amazing day .\n",
      "trg = that should complete an amazing day .\n",
      "out = that should complete that the day .\n",
      "---\n",
      "src = i can listen to the radio and sometimes can listen to thai music which i brought some cassets with me from thailand .\n",
      "trg = i can listen to the radio and sometimes can listen to thai music , as i brought some cassettes with me from thailand .\n",
      "out = i can listen to the radio and sometimes can listen to <unk> music , which i brought some <unk> with me from politics .\n",
      "---\n",
      "src = then we all take you and the students to the airport .\n",
      "trg = then we will all take you and the students to the airport .\n",
      "out = then we all all you and the students to the stores .\n",
      "---\n",
      "src = the car has largely affected my own life .\n",
      "trg = the car has significantly affected my life .\n",
      "out = the car has brought affected my own life .\n",
      "---\n",
      "src = who could have invented such a useful item ?\n",
      "trg = who could have invented such a useful item ?\n",
      "out = who could have a such such a useful ?\n",
      "---\n",
      "src = i hope that all the information you need would be answered satisfactorialy .\n",
      "trg = i hope that all the information you need will be provided satisfactorily .\n",
      "out = i hope that all the information you need would be answered <unk> .\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"testing inference\")\n",
    "inference(test_set, src_field, trg_field, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 56.61\n"
     ]
    }
   ],
   "source": [
    "bleu = calculate_bleu(test_set, src_field, trg_field, model)\n",
    "print(f'BLEU score = {bleu*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
